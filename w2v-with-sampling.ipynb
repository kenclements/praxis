{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data management tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "# #Vectorization and Tokenizing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import utils\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "\n",
    "# #Models\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# #Metrics and Testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/logon.csv')\n",
    "df2 = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/device.csv')\n",
    "df3 = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/email.csv')\n",
    "df4 = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/file.csv')\n",
    "df5 = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/http.csv')\n",
    "ans = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/answers/answers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = datetime.date(2010,7,1)\n",
    "END = datetime.date(2010,8, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['date'] = pd.to_datetime(df1['date'])\n",
    "df2['date'] = pd.to_datetime(df2['date'])\n",
    "df3['date'] = pd.to_datetime(df3['date'])\n",
    "df4['date'] = pd.to_datetime(df4['date'])\n",
    "df5['date'] = pd.to_datetime(df5['date'])\n",
    "\n",
    "df1['day_date'] = pd.to_datetime(df1['date']).dt.date\n",
    "df2['day_date'] = pd.to_datetime(df2['date']).dt.date\n",
    "df3['day_date'] = pd.to_datetime(df3['date']).dt.date\n",
    "df4['day_date'] = pd.to_datetime(df4['date']).dt.date\n",
    "df5['day_date'] = pd.to_datetime(df5['date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['activity'] = \"email\"\n",
    "df4['activity'] = \"file\"\n",
    "df5['activity'] = \"http\"\n",
    "\n",
    "df1_bs = df1[((df1['day_date'] >= START) & (df1['day_date'] <= END))]\n",
    "df2_bs = df2[((df2['day_date'] >= START) & (df2['day_date'] <= END))]\n",
    "df3_bs = df3[((df3['day_date'] >= START) & (df3['day_date'] <= END))]\n",
    "df4_bs = df4[((df4['day_date'] >= START) & (df4['day_date'] <= END))]\n",
    "df5_bs = df5[((df5['day_date'] >= START) & (df5['day_date'] <= END))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ken.clements/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ken.clements/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ken.clements/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "def prepare_text(pretext):\n",
    "\n",
    "    # tokenize mission.\n",
    "    tokens = word_tokenize(pretext, language=\"english\", preserve_line=True)\n",
    "\n",
    "    # Parts of speech (POS) tag tokens.\n",
    "    token_tag = pos_tag(tokens)\n",
    "    \n",
    "    # Only include some of the POS tags.\n",
    "    include_tags = ['VBN', 'VBD', 'JJ', 'JJS', 'JJR', 'CD', 'NN', 'NNS', 'NNP', 'NNPS']\n",
    "    filtered_tokens = (tok for tok, tag in token_tag if tag in include_tags)    \n",
    "    \n",
    "    #stem words.\n",
    "    #stemmed_tokens = (lancaster.stem(tok) for tok in filtered_tokens)\n",
    "\n",
    "    #lemm words.\n",
    "    #lemmed_words = (lemmatizer.lemmatize(tok) for tok in stemmed_tokens)\n",
    "\n",
    "    #lower_words = [str.lower(tok) for tok in lemmed_words]\n",
    "    lower_words = [str.lower(tok) for tok in filtered_tokens]\n",
    "\n",
    "    #x = cv.fit_transform(tokens).toarray()\n",
    "\n",
    "    #joined = \", \".join(lower_words)\n",
    "\n",
    "    return(lower_words)\n",
    "\n",
    "#ps = PorterStemmer()\n",
    "#lancaster = LancasterStemmer()\n",
    "#lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/svf85_b50jvfz4dvm3yf5h0c0000gq/T/ipykernel_8124/2382823816.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3_bs['df3tokenized'] = df3_bs['content'].apply(prepare_text)\n"
     ]
    }
   ],
   "source": [
    "df3_bs['df3tokenized'] = df3_bs['content'].apply(prepare_text)\n",
    "# df4_bs['df4tokenized'] = df4_bs['content'].apply(prepare_text)\n",
    "# df5_bs['df5tokenized'] = df5_bs['content'].apply(prepare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat([df1_bs, df2_bs, df3_bs])\n",
    "# df_merged = pd.concat([df1_bs, df2_bs, df3_bs, df4_bs, df5_bs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['label'] = 'normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.loc[df_merged['id'].isin(ans['id']),['label']] = 'bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merged.dropna(subset=['df3tokenized'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['df3tokenized','user']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train.apply(lambda r: TaggedDocument(words=r['df3tokenized'], tags=r['user']), axis=1)\n",
    "test_tagged = test.apply(lambda r: TaggedDocument(words=r['df3tokenized'], tags=r['user']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=5, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 228943/228943 [00:00<00:00, 4045144.14it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 228943/228943 [00:00<00:00, 4314150.41it/s]\n",
      "100%|██████████| 228943/228943 [00:00<00:00, 5079378.69it/s]\n",
      "100%|██████████| 228943/228943 [00:00<00:00, 4748831.86it/s]\n",
      "100%|██████████| 228943/228943 [00:00<00:00, 4866543.05it/s]\n",
      "100%|██████████| 228943/228943 [00:00<00:00, 4963848.75it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags, model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5, max_iter=100)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.9994904147005167\n",
      "Testing F1 score: 0.9992356869866146\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0    50]\n",
      " [    0 98069]]\n"
     ]
    }
   ],
   "source": [
    "cf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "blsmote = BorderlineSMOTE()\n",
    "X_train_blsmote, y_train_blsmote = blsmote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train_blsmote, y_train_blsmote)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.981226877567036\n",
      "Testing F1 score: 0.9900374613589615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   41     9]\n",
      " [ 1833 96236]]\n"
     ]
    }
   ],
   "source": [
    "cf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "adasyn = ADASYN()\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train_adasyn, y_train_adasyn)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score, f1_score\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTesting accuracy \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m accuracy_score(y_test, y_pred))\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTesting recall \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m recall_score(y_test, y_pred))\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTesting F1 score: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(f1_score(y_test, y_pred, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m'\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing recall %s' % recall_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   42     8]\n",
      " [ 1980 96089]]\n"
     ]
    }
   ],
   "source": [
    "cf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df_merged['day_date']\n",
    "dates = dates.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwow = []\n",
    "for d in dates:\n",
    "    dayevents = df_merged.loc[(df_merged['day_date'] == d)]\n",
    "    users = dayevents['user']\n",
    "    users = users.unique()\n",
    "    for u in users:\n",
    "        userd = dayevents[(dayevents['user'] == u)]\n",
    "        date = d\n",
    "        username = u\n",
    "        login_count = userd[userd['activity'] == 'Logon'].shape[0]\n",
    "        logout_count = userd[userd['activity'] == 'Logoff'].shape[0]\n",
    "        email_count = userd[userd['to'].notnull()].shape[0]\n",
    "        file_count = userd[userd['filename'].notnull()].shape[0]\n",
    "        http_count = userd[userd['url'].notnull()].shape[0]\n",
    "        start_time = userd['date'].iloc[0]\n",
    "        end_time = userd['date'].iloc[-1]\n",
    "\n",
    "        if (userd[(userd['label'] == 'bad')].shape[0] > 0): label = 1\n",
    "        else: label = 0\n",
    "        dfwow.append([date, username, login_count, logout_count, email_count, file_count, http_count, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(dfwow, columns=['date', 'username', 'login_count', 'logout_count', 'email_count', 'file_count', 'http_count', 'tfidf3_mean', 'tfidf4_mean', 'tfidf5_mean', 'label'])\n",
    "final = final.dropna(subset=['tfidf5_mean', 'tfidf3_mean', 'tfidf4_mean'])\n",
    "final['date'] = (final['date'] - final['date'].min())  / np.timedelta64(1,'D')\n",
    "label_encoder = LabelEncoder()\n",
    "final['username'] = label_encoder.fit_transform(final['username'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final.loc[:, final.columns != 'label']\n",
    "y = final['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_adasyn = ADASYN()\n",
    "X_final_adasyn, y_final_adasyn = final_adasyn.fit_resample(X, y)\n",
    "final_adasyn = X_final_adasyn.copy()\n",
    "final_adasyn['label'] = y_final_adasyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,stratify=y)\n",
    "\n",
    "classifier = LogisticRegression(max_iter=3)\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "predicted = classifier.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "#print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Praxis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
