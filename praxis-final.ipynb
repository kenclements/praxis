{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import datetime\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score as shs\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold, train_test_split, TimeSeriesSplit\n",
    "import xgboost\n",
    "import shap\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, Dense, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/logon.csv')\n",
    "# df2 = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/device.csv')\n",
    "# df3 = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/email.csv')\n",
    "# df4 = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/file.csv')\n",
    "# df5 = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/http.csv')\n",
    "# ans = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/answers/answers.csv')\n",
    "# ans = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/answers/answers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psy = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/psychometric.csv')\n",
    "ldap = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/ldap/2009-12.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the elbow method to determine the number of clusters for Psychometric combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 11)  # Trying K from 1 to 10 clusters\n",
    "inertia_values = []\n",
    "\n",
    "# Calculate the sum of squared distances (inertia) for each K\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(psy[['C', 'A', 'N']])\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_values, inertia_values, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Sum of Squared Distances (Inertia)')\n",
    "plt.title('Elbow Plot for K-means Clustering C, A, and N Factors')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette values show A, C, anc N are slightly better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(psy[['C', 'A', 'E', 'N']])\n",
    "cluster_assignments = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "sil_score = shs(psy[['C', 'A', 'E', 'N']], cluster_assignments)\n",
    "sil_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n",
    "\n",
    "visualizer.fit(psy[['C', 'A', 'N']])        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 11)\n",
    "inertia_values = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(psy[['O', 'A', 'N']])\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_values, inertia_values, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Sum of Squared Distances (Inertia)')\n",
    "plt.title('Elbow Plot for K-means Clustering')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(psy[['C', 'A', 'N']])\n",
    "cluster_assignments = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "sil_score = shs(psy[['C', 'A', 'N']], cluster_assignments)\n",
    "sil_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n",
    "visualizer.fit(psy[['C', 'A', 'N']])\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 11)  # Trying K from 1 to 10 clusters\n",
    "inertia_values = []\n",
    "\n",
    "# Calculate the sum of squared distances (inertia) for each K\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(psy[['O', 'C', 'N']])\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_values, inertia_values, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Sum of Squared Distances (Inertia)')\n",
    "plt.title('Elbow Plot for K-means Clustering')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(psy[['O', 'C', 'N']])\n",
    "cluster_assignments = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "sil_score = shs(psy[['O', 'C', 'N']], cluster_assignments)\n",
    "sil_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n",
    "\n",
    "visualizer.fit(psy[['O', 'C', 'N']])        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 11)  # Trying K from 1 to 10 clusters\n",
    "inertia_values = []\n",
    "\n",
    "# Calculate the sum of squared distances (inertia) for each K\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(psy[['O', 'E', 'N']])\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_values, inertia_values, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Sum of Squared Distances (Inertia)')\n",
    "plt.title('Elbow Plot for K-means Clustering')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(psy[['O', 'E', 'N']])\n",
    "cluster_assignments = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "sil_score = shs(psy[['O', 'E', 'N']], cluster_assignments)\n",
    "sil_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n",
    "\n",
    "visualizer.fit(psy[['O', 'E', 'N']])\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "\n",
    "ax.scatter(psy['A'], psy['C'], psy['N'], c=cluster_assignments, cmap='rainbow')\n",
    "\n",
    "ax.set_xlabel('Agreeableness')\n",
    "ax.set_ylabel('Conscientiousness')\n",
    "ax.set_zlabel('Narcissism')\n",
    "ax.set_title('Psychometric Classification')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign psychometric groups to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psy['cluster'] = cluster_assignments\n",
    "ldap = pd.merge(ldap, psy[['user_id', 'cluster']], on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the first date that a user exhibits insider behavior in a separate dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_detected = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/earliest.csv')\n",
    "date_detected['datetime'] = pd.to_datetime(date_detected['date'], infer_datetime_format=True)\n",
    "date_detected['day_date'] = pd.to_datetime(date_detected['datetime']).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign activity types to records in these dataframes as they are not identified within the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['activity'] = \"email\"\n",
    "df4['activity'] = \"file\"\n",
    "df5['activity'] = \"http\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some date pre-processing activities for time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = datetime.date(2010,1,1)\n",
    "END = datetime.date(2011,12,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['date'] = pd.to_datetime(df1['date'], infer_datetime_format=True)\n",
    "df1['day_date'] = pd.to_datetime(df1['date']).dt.date\n",
    "\n",
    "df2['date'] = pd.to_datetime(df2['date'], infer_datetime_format=True)\n",
    "df2['day_date'] = pd.to_datetime(df2['date']).dt.date\n",
    "\n",
    "df3['date'] = pd.to_datetime(df3['date'], infer_datetime_format=True)\n",
    "df3['day_date'] = pd.to_datetime(df3['date']).dt.date\n",
    "\n",
    "df4['date'] = pd.to_datetime(df4['date'], infer_datetime_format=True)\n",
    "df4['day_date'] = pd.to_datetime(df4['date']).dt.date\n",
    "\n",
    "df5['date'] = pd.to_datetime(df5['date'], infer_datetime_format=True)\n",
    "df5['day_date'] = pd.to_datetime(df5['date']).dt.date\n",
    "\n",
    "df1 = df1.loc[((df1['day_date'] >= START) & (df1['day_date'] <= END))]\n",
    "df2 = df2.loc[((df2['day_date'] >= START) & (df2['day_date'] <= END))]\n",
    "df3 = df3.loc[((df3['day_date'] >= START) & (df3['day_date'] <= END))]\n",
    "df4 = df4.loc[((df4['day_date'] >= START) & (df4['day_date'] <= END))]\n",
    "df5 = df5.loc[((df5['day_date'] >= START) & (df5['day_date'] <= END))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove text fields to save on memory.  Can be changed if NLP features are added in future research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['content'].fillna('', inplace=True)\n",
    "# df5['tokenized'] = df5['content'].apply(prepare_text)\n",
    "df3[\"content\"] = ''\n",
    "df4[\"content\"] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge into single dataframe, assign labels to records, and merge LDAP records into each activity record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_range = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)\n",
    "df_time_range['label'] = 'normal'\n",
    "df_time_range.loc[df_time_range['id'].isin(ans['id']),['label']] = 'abnormal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userEvents = df_time_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the daily activity vector with 20 features modified from prior research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(start=START, end=END)\n",
    "START_TIME = datetime.time(7, 30)\n",
    "END_TIME = datetime.time(17, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = userEvents['user'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateActivity = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = userEvents.loc[(userEvents['day_date'] == d.date())]\n",
    "    for u in users:\n",
    "        activityList = []\n",
    "        userdf = daysEvents.loc[(daysEvents['user'] == u)]\n",
    "        userdf.sort_values(by='date', inplace=True)\n",
    "        userdf.reset_index(drop=True, inplace=True)\n",
    "        httpDailyCorpus = userdf.loc[userdf['activity'] == 'http']\n",
    "        combined_start = datetime.datetime.combine(d, START_TIME)\n",
    "        combined_end = datetime.datetime.combine(d, END_TIME)\n",
    "        pcs = userdf['pc'].unique()\n",
    "        if (userdf.loc[userdf['activity'] == 'Logon'].shape[0] > 0):\n",
    "            l_df = userdf.loc[((userdf['activity'] == 'Logon') | (userdf['activity'] == 'Logoff'))]\n",
    "            l_df.reset_index(drop=True, inplace=True)\n",
    "            l1 = (combined_start - l_df['date'].loc[l_df['activity'] == 'Logon'].iloc[0]).total_seconds()\n",
    "            l2 = (l_df['date'].loc[l_df['activity'] == 'Logon'].iloc[-1] - combined_end).total_seconds()\n",
    "            l3 = (combined_start - l_df['date'].loc[(l_df['activity'] == 'Logon') & (l_df['date'] <  combined_start)]).mean().total_seconds()\n",
    "            if np.isnan(l3): l3 = 0\n",
    "            l4 = (l_df['date'].loc[(l_df['activity'] == 'Logon') & (l_df['date'] >  combined_end)] - combined_end).mean().total_seconds()\n",
    "            if np.isnan(l4): l4 = 0 \n",
    "            l7 = l_df['pc'].unique().shape[0]\n",
    "            l8 = l_df['pc'].loc[(l_df['activity'] == 'Logon') & ((l_df['date'] >  combined_end) | (l_df['date'] < combined_start))].shape[0]\n",
    "            session_lengths = []\n",
    "            for p in pcs:\n",
    "                p_df = l_df.loc[(l_df['pc'] == p)]\n",
    "                initial_login = True\n",
    "                for index, row in p_df.iterrows():\n",
    "                    if initial_login == True:\n",
    "                        if (row['activity'] == 'Logon'):\n",
    "                            session_start = row['date']\n",
    "                            initial_login = False\n",
    "                    if (row['activity'] == 'Logoff'):\n",
    "                        session_stop = row['date']\n",
    "                        initial_login = True\n",
    "                        length = (session_stop - session_start).total_seconds()\n",
    "                        if ((session_stop < combined_start) | (session_start > combined_end)):\n",
    "                            session_lengths.append(length)\n",
    "            if len(session_lengths) > 0:\n",
    "                l9 = sum(session_lengths)/len(session_lengths)\n",
    "            else:\n",
    "                l9 = 0\n",
    "            activityList = [l1, l2, l3, l4, l7, l8, l9]\n",
    "            dateActivity.append([d, u, activityList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(dateActivity, columns=['date', 'user', 'activityList'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame(final['activityList'].tolist())\n",
    "df_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = final.join(df_temp)\n",
    "df_final.rename(columns={0: \"l1\"}, inplace=True)\n",
    "df_final.rename(columns={1: \"l2\"}, inplace=True)\n",
    "df_final.rename(columns={2: \"l3\"}, inplace=True)\n",
    "df_final.rename(columns={3: \"l4\"}, inplace=True)\n",
    "df_final.rename(columns={4: \"l7\"}, inplace=True)\n",
    "df_final.rename(columns={5: \"l8\"}, inplace=True)\n",
    "df_final.rename(columns={6: \"l9\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_df = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/with_users_dates.csv')\n",
    "intro_df['user'] = intro_df['user_id']\n",
    "intro_df['date'] = pd.to_datetime(intro_df['date'])\n",
    "df_final2 = pd.merge(df_final, intro_df, on=['user', 'date'], how='left')\n",
    "df_final = df_final2\n",
    "df_final['label'] = 0\n",
    "df_final.loc[df_final['insider'] != 0, 'label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final.to_pickle('to_role_measurement.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_pickle('to_role_measurement.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_from_intro = ['n_allact','allact_n-pc0','allact_n-pc1','allact_n-pc2','allact_n-pc3','n_workhourallact','workhourallact_n-pc0','workhourallact_n-pc1','workhourallact_n-pc2','workhourallact_n-pc3','n_afterhourallact','afterhourallact_n-pc0','afterhourallact_n-pc1','afterhourallact_n-pc2','afterhourallact_n-pc3','n_logon','logon_n-pc0','logon_n-pc1','logon_n-pc2','logon_n-pc3','n_workhourlogon','workhourlogon_n-pc0','workhourlogon_n-pc1','workhourlogon_n-pc2','workhourlogon_n-pc3','n_afterhourlogon','afterhourlogon_n-pc0','afterhourlogon_n-pc1','afterhourlogon_n-pc2','afterhourlogon_n-pc3','n_usb','usb_n-pc0','usb_n-pc1','usb_n-pc2','usb_n-pc3','n_workhourusb','workhourusb_n-pc0','workhourusb_n-pc1','workhourusb_n-pc2','workhourusb_n-pc3','n_afterhourusb','afterhourusb_n-pc0','afterhourusb_n-pc1','afterhourusb_n-pc2','afterhourusb_n-pc3','n_file','file_mean_file_len','file_mean_file_depth','file_mean_file_nwords','file_n-disk0','file_n-disk1','file_n-pc0','file_n-pc1','file_n-pc2','file_n-pc3','file_n_compf','file_compf_mean_file_len','file_compf_mean_file_depth','file_compf_mean_file_nwords','file_compf_n-disk0','file_compf_n-disk1','file_compf_n-pc0','file_compf_n-pc1','file_compf_n-pc2','file_compf_n-pc3','file_n_phof','file_phof_mean_file_len','file_phof_mean_file_depth','file_phof_mean_file_nwords','file_phof_n-disk0','file_phof_n-disk1','file_phof_n-pc0','file_phof_n-pc1','file_phof_n-pc2','file_phof_n-pc3','file_n_docf','file_docf_mean_file_len','file_docf_mean_file_depth','file_docf_mean_file_nwords','file_docf_n-disk0','file_docf_n-disk1','file_docf_n-pc0','file_docf_n-pc1','file_docf_n-pc2','file_docf_n-pc3','file_n_txtf','file_txtf_mean_file_len','file_txtf_mean_file_depth','file_txtf_mean_file_nwords','file_txtf_n-disk0','file_txtf_n-disk1','file_txtf_n-pc0','file_txtf_n-pc1','file_txtf_n-pc2','file_txtf_n-pc3','file_n_exef','file_exef_mean_file_len','file_exef_mean_file_depth','file_exef_mean_file_nwords','file_exef_n-disk0','file_exef_n-disk1','file_exef_n-pc0','file_exef_n-pc1','file_exef_n-pc2','file_exef_n-pc3','n_workhourfile','workhourfile_mean_file_len','workhourfile_mean_file_depth','workhourfile_mean_file_nwords','workhourfile_n-disk0','workhourfile_n-disk1','workhourfile_n-pc0','workhourfile_n-pc1','workhourfile_n-pc2','workhourfile_n-pc3','workhourfile_n_compf','workhourfile_compf_mean_file_len','workhourfile_compf_mean_file_depth','workhourfile_compf_mean_file_nwords','workhourfile_compf_n-disk0','workhourfile_compf_n-disk1','workhourfile_compf_n-pc0','workhourfile_compf_n-pc1','workhourfile_compf_n-pc2','workhourfile_compf_n-pc3','workhourfile_n_phof','workhourfile_phof_mean_file_len','workhourfile_phof_mean_file_depth','workhourfile_phof_mean_file_nwords','workhourfile_phof_n-disk0','workhourfile_phof_n-disk1','workhourfile_phof_n-pc0','workhourfile_phof_n-pc1','workhourfile_phof_n-pc2','workhourfile_phof_n-pc3','workhourfile_n_docf','workhourfile_docf_mean_file_len','workhourfile_docf_mean_file_depth','workhourfile_docf_mean_file_nwords','workhourfile_docf_n-disk0','workhourfile_docf_n-disk1','workhourfile_docf_n-pc0','workhourfile_docf_n-pc1','workhourfile_docf_n-pc2','workhourfile_docf_n-pc3','workhourfile_n_txtf','workhourfile_txtf_mean_file_len','workhourfile_txtf_mean_file_depth','workhourfile_txtf_mean_file_nwords','workhourfile_txtf_n-disk0','workhourfile_txtf_n-disk1','workhourfile_txtf_n-pc0','workhourfile_txtf_n-pc1','workhourfile_txtf_n-pc2','workhourfile_txtf_n-pc3','workhourfile_n_exef','workhourfile_exef_mean_file_len','workhourfile_exef_mean_file_depth','workhourfile_exef_mean_file_nwords','workhourfile_exef_n-disk0','workhourfile_exef_n-disk1','workhourfile_exef_n-pc0','workhourfile_exef_n-pc1','workhourfile_exef_n-pc2','workhourfile_exef_n-pc3','n_afterhourfile','afterhourfile_mean_file_len','afterhourfile_mean_file_depth','afterhourfile_mean_file_nwords','afterhourfile_n-disk0','afterhourfile_n-disk1','afterhourfile_n-pc0','afterhourfile_n-pc1','afterhourfile_n-pc2','afterhourfile_n-pc3','afterhourfile_n_compf','afterhourfile_compf_mean_file_len','afterhourfile_compf_mean_file_depth','afterhourfile_compf_mean_file_nwords','afterhourfile_compf_n-disk0','afterhourfile_compf_n-disk1','afterhourfile_compf_n-pc0','afterhourfile_compf_n-pc1','afterhourfile_compf_n-pc2','afterhourfile_compf_n-pc3','afterhourfile_n_phof','afterhourfile_phof_mean_file_len','afterhourfile_phof_mean_file_depth','afterhourfile_phof_mean_file_nwords','afterhourfile_phof_n-disk0','afterhourfile_phof_n-disk1','afterhourfile_phof_n-pc0','afterhourfile_phof_n-pc1','afterhourfile_phof_n-pc2','afterhourfile_phof_n-pc3','afterhourfile_n_docf','afterhourfile_docf_mean_file_len','afterhourfile_docf_mean_file_depth','afterhourfile_docf_mean_file_nwords','afterhourfile_docf_n-disk0','afterhourfile_docf_n-disk1','afterhourfile_docf_n-pc0','afterhourfile_docf_n-pc1','afterhourfile_docf_n-pc2','afterhourfile_docf_n-pc3','afterhourfile_n_txtf','afterhourfile_txtf_mean_file_len','afterhourfile_txtf_mean_file_depth','afterhourfile_txtf_mean_file_nwords','afterhourfile_txtf_n-disk0','afterhourfile_txtf_n-disk1','afterhourfile_txtf_n-pc0','afterhourfile_txtf_n-pc1','afterhourfile_txtf_n-pc2','afterhourfile_txtf_n-pc3','afterhourfile_n_exef','afterhourfile_exef_mean_file_len','afterhourfile_exef_mean_file_depth','afterhourfile_exef_mean_file_nwords','afterhourfile_exef_n-disk0','afterhourfile_exef_n-disk1','afterhourfile_exef_n-pc0','afterhourfile_exef_n-pc1','afterhourfile_exef_n-pc2','afterhourfile_exef_n-pc3','n_email','email_mean_n_des','email_mean_n_atts','email_mean_n_exdes','email_mean_n_bccdes','email_mean_email_size','email_mean_email_text_slen','email_mean_email_text_nwords','email_n-Xemail1','email_n-exbccmail1','email_n-pc0','email_n-pc1','email_n-pc2','email_n-pc3','n_workhouremail','workhouremail_mean_n_des','workhouremail_mean_n_atts','workhouremail_mean_n_exdes','workhouremail_mean_n_bccdes','workhouremail_mean_email_size','workhouremail_mean_email_text_slen','workhouremail_mean_email_text_nwords','workhouremail_n-Xemail1','workhouremail_n-exbccmail1','workhouremail_n-pc0','workhouremail_n-pc1','workhouremail_n-pc2','workhouremail_n-pc3','n_afterhouremail','afterhouremail_mean_n_des','afterhouremail_mean_n_atts','afterhouremail_mean_n_exdes','afterhouremail_mean_n_bccdes','afterhouremail_mean_email_size','afterhouremail_mean_email_text_slen','afterhouremail_mean_email_text_nwords','afterhouremail_n-Xemail1','afterhouremail_n-exbccmail1','afterhouremail_n-pc0','afterhouremail_n-pc1','afterhouremail_n-pc2','afterhouremail_n-pc3','n_http','http_mean_url_len','http_mean_url_depth','http_mean_http_c_len','http_mean_http_c_nwords','http_n-pc0','http_n-pc1','http_n-pc2','http_n-pc3','http_n_otherf','http_otherf_mean_url_len','http_otherf_mean_url_depth','http_otherf_mean_http_c_len','http_otherf_mean_http_c_nwords','http_otherf_n-pc0','http_otherf_n-pc1','http_otherf_n-pc2','http_otherf_n-pc3','http_n_socnetf','http_socnetf_mean_url_len','http_socnetf_mean_url_depth','http_socnetf_mean_http_c_len','http_socnetf_mean_http_c_nwords','http_socnetf_n-pc0','http_socnetf_n-pc1','http_socnetf_n-pc2','http_socnetf_n-pc3','http_n_cloudf','http_cloudf_mean_url_len','http_cloudf_mean_url_depth','http_cloudf_mean_http_c_len','http_cloudf_mean_http_c_nwords','http_cloudf_n-pc0','http_cloudf_n-pc1','http_cloudf_n-pc2','http_cloudf_n-pc3','http_n_jobf','http_jobf_mean_url_len','http_jobf_mean_url_depth','http_jobf_mean_http_c_len','http_jobf_mean_http_c_nwords','http_jobf_n-pc0','http_jobf_n-pc1','http_jobf_n-pc2','http_jobf_n-pc3','http_n_leakf','http_leakf_mean_url_len','http_leakf_mean_url_depth','http_leakf_mean_http_c_len','http_leakf_mean_http_c_nwords','http_leakf_n-pc0','http_leakf_n-pc1','http_leakf_n-pc2','http_leakf_n-pc3','http_n_hackf','http_hackf_mean_url_len','http_hackf_mean_url_depth','http_hackf_mean_http_c_len','http_hackf_mean_http_c_nwords','http_hackf_n-pc0','http_hackf_n-pc1','http_hackf_n-pc2','http_hackf_n-pc3','n_workhourhttp','workhourhttp_mean_url_len','workhourhttp_mean_url_depth','workhourhttp_mean_http_c_len','workhourhttp_mean_http_c_nwords','workhourhttp_n-pc0','workhourhttp_n-pc1','workhourhttp_n-pc2','workhourhttp_n-pc3','workhourhttp_n_otherf','workhourhttp_otherf_mean_url_len','workhourhttp_otherf_mean_url_depth','workhourhttp_otherf_mean_http_c_len','workhourhttp_otherf_mean_http_c_nwords','workhourhttp_otherf_n-pc0','workhourhttp_otherf_n-pc1','workhourhttp_otherf_n-pc2','workhourhttp_otherf_n-pc3','workhourhttp_n_socnetf','workhourhttp_socnetf_mean_url_len','workhourhttp_socnetf_mean_url_depth','workhourhttp_socnetf_mean_http_c_len','workhourhttp_socnetf_mean_http_c_nwords','workhourhttp_socnetf_n-pc0','workhourhttp_socnetf_n-pc1','workhourhttp_socnetf_n-pc2','workhourhttp_socnetf_n-pc3','workhourhttp_n_cloudf','workhourhttp_cloudf_mean_url_len','workhourhttp_cloudf_mean_url_depth','workhourhttp_cloudf_mean_http_c_len','workhourhttp_cloudf_mean_http_c_nwords','workhourhttp_cloudf_n-pc0','workhourhttp_cloudf_n-pc1','workhourhttp_cloudf_n-pc2','workhourhttp_cloudf_n-pc3','workhourhttp_n_jobf','workhourhttp_jobf_mean_url_len','workhourhttp_jobf_mean_url_depth','workhourhttp_jobf_mean_http_c_len','workhourhttp_jobf_mean_http_c_nwords','workhourhttp_jobf_n-pc0','workhourhttp_jobf_n-pc1','workhourhttp_jobf_n-pc2','workhourhttp_jobf_n-pc3','workhourhttp_n_leakf','workhourhttp_leakf_mean_url_len','workhourhttp_leakf_mean_url_depth','workhourhttp_leakf_mean_http_c_len','workhourhttp_leakf_mean_http_c_nwords','workhourhttp_leakf_n-pc0','workhourhttp_leakf_n-pc1','workhourhttp_leakf_n-pc2','workhourhttp_leakf_n-pc3','workhourhttp_n_hackf','workhourhttp_hackf_mean_url_len','workhourhttp_hackf_mean_url_depth','workhourhttp_hackf_mean_http_c_len','workhourhttp_hackf_mean_http_c_nwords','workhourhttp_hackf_n-pc0','workhourhttp_hackf_n-pc1','workhourhttp_hackf_n-pc2','workhourhttp_hackf_n-pc3','n_afterhourhttp','afterhourhttp_mean_url_len','afterhourhttp_mean_url_depth','afterhourhttp_mean_http_c_len','afterhourhttp_mean_http_c_nwords','afterhourhttp_n-pc0','afterhourhttp_n-pc1','afterhourhttp_n-pc2','afterhourhttp_n-pc3','afterhourhttp_n_otherf','afterhourhttp_otherf_mean_url_len','afterhourhttp_otherf_mean_url_depth','afterhourhttp_otherf_mean_http_c_len','afterhourhttp_otherf_mean_http_c_nwords','afterhourhttp_otherf_n-pc0','afterhourhttp_otherf_n-pc1','afterhourhttp_otherf_n-pc2','afterhourhttp_otherf_n-pc3','afterhourhttp_n_socnetf','afterhourhttp_socnetf_mean_url_len','afterhourhttp_socnetf_mean_url_depth','afterhourhttp_socnetf_mean_http_c_len','afterhourhttp_socnetf_mean_http_c_nwords','afterhourhttp_socnetf_n-pc0','afterhourhttp_socnetf_n-pc1','afterhourhttp_socnetf_n-pc2','afterhourhttp_socnetf_n-pc3','afterhourhttp_n_cloudf','afterhourhttp_cloudf_mean_url_len','afterhourhttp_cloudf_mean_url_depth','afterhourhttp_cloudf_mean_http_c_len','afterhourhttp_cloudf_mean_http_c_nwords','afterhourhttp_cloudf_n-pc0','afterhourhttp_cloudf_n-pc1','afterhourhttp_cloudf_n-pc2','afterhourhttp_cloudf_n-pc3','afterhourhttp_n_jobf','afterhourhttp_jobf_mean_url_len','afterhourhttp_jobf_mean_url_depth','afterhourhttp_jobf_mean_http_c_len','afterhourhttp_jobf_mean_http_c_nwords','afterhourhttp_jobf_n-pc0','afterhourhttp_jobf_n-pc1','afterhourhttp_jobf_n-pc2','afterhourhttp_jobf_n-pc3','afterhourhttp_n_leakf','afterhourhttp_leakf_mean_url_len','afterhourhttp_leakf_mean_url_depth','afterhourhttp_leakf_mean_http_c_len','afterhourhttp_leakf_mean_http_c_nwords','afterhourhttp_leakf_n-pc0','afterhourhttp_leakf_n-pc1','afterhourhttp_leakf_n-pc2','afterhourhttp_leakf_n-pc3','afterhourhttp_n_hackf','afterhourhttp_hackf_mean_url_len','afterhourhttp_hackf_mean_url_depth','afterhourhttp_hackf_mean_http_c_len','afterhourhttp_hackf_mean_http_c_nwords','afterhourhttp_hackf_n-pc0','afterhourhttp_hackf_n-pc1','afterhourhttp_hackf_n-pc2','afterhourhttp_hackf_n-pc3'\n",
    "]\n",
    "\n",
    "features_from_chap = ['l1', 'l2', 'l3', 'l4', 'l7', 'l8', 'l9']\n",
    "features = features_from_chap + features_from_intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = df_final\n",
    "bad_roles = importance_df['role'].loc[importance_df['label'] == 1].unique()\n",
    "importance_df = importance_df.loc[importance_df['role'].isin(bad_roles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = importance_df['user'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = pd.DataFrame()\n",
    "\n",
    "roll_period = 14\n",
    "\n",
    "mean_features = []\n",
    "median_features = []\n",
    "iqr_features = []\n",
    "std_features = []\n",
    "\n",
    "for u in tqdm(users):\n",
    "    df_u = importance_df.loc[importance_df['user'] == u]\n",
    "    df_u.sort_values(by='date', inplace=True, ignore_index=True)\n",
    "    df_u.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    for j in features:\n",
    "        df_u['rolling_mean_' + j] = df_u[j].rolling(roll_period).mean()\n",
    "        mean_features.append('rolling_mean_' + j)\n",
    "\n",
    "    reconstructed = pd.concat([reconstructed, df_u]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance_df.to_pickle('importance.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import datetime\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import silhouette_score as shs\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold, train_test_split, TimeSeriesSplit\n",
    "import xgboost\n",
    "import shap\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_from_intro = ['n_allact','allact_n-pc0','allact_n-pc1','allact_n-pc2','allact_n-pc3','n_workhourallact','workhourallact_n-pc0','workhourallact_n-pc1','workhourallact_n-pc2','workhourallact_n-pc3','n_afterhourallact','afterhourallact_n-pc0','afterhourallact_n-pc1','afterhourallact_n-pc2','afterhourallact_n-pc3','n_logon','logon_n-pc0','logon_n-pc1','logon_n-pc2','logon_n-pc3','n_workhourlogon','workhourlogon_n-pc0','workhourlogon_n-pc1','workhourlogon_n-pc2','workhourlogon_n-pc3','n_afterhourlogon','afterhourlogon_n-pc0','afterhourlogon_n-pc1','afterhourlogon_n-pc2','afterhourlogon_n-pc3','n_usb','usb_n-pc0','usb_n-pc1','usb_n-pc2','usb_n-pc3','n_workhourusb','workhourusb_n-pc0','workhourusb_n-pc1','workhourusb_n-pc2','workhourusb_n-pc3','n_afterhourusb','afterhourusb_n-pc0','afterhourusb_n-pc1','afterhourusb_n-pc2','afterhourusb_n-pc3','n_file','file_mean_file_len','file_mean_file_depth','file_mean_file_nwords','file_n-disk0','file_n-disk1','file_n-pc0','file_n-pc1','file_n-pc2','file_n-pc3','file_n_compf','file_compf_mean_file_len','file_compf_mean_file_depth','file_compf_mean_file_nwords','file_compf_n-disk0','file_compf_n-disk1','file_compf_n-pc0','file_compf_n-pc1','file_compf_n-pc2','file_compf_n-pc3','file_n_phof','file_phof_mean_file_len','file_phof_mean_file_depth','file_phof_mean_file_nwords','file_phof_n-disk0','file_phof_n-disk1','file_phof_n-pc0','file_phof_n-pc1','file_phof_n-pc2','file_phof_n-pc3','file_n_docf','file_docf_mean_file_len','file_docf_mean_file_depth','file_docf_mean_file_nwords','file_docf_n-disk0','file_docf_n-disk1','file_docf_n-pc0','file_docf_n-pc1','file_docf_n-pc2','file_docf_n-pc3','file_n_txtf','file_txtf_mean_file_len','file_txtf_mean_file_depth','file_txtf_mean_file_nwords','file_txtf_n-disk0','file_txtf_n-disk1','file_txtf_n-pc0','file_txtf_n-pc1','file_txtf_n-pc2','file_txtf_n-pc3','file_n_exef','file_exef_mean_file_len','file_exef_mean_file_depth','file_exef_mean_file_nwords','file_exef_n-disk0','file_exef_n-disk1','file_exef_n-pc0','file_exef_n-pc1','file_exef_n-pc2','file_exef_n-pc3','n_workhourfile','workhourfile_mean_file_len','workhourfile_mean_file_depth','workhourfile_mean_file_nwords','workhourfile_n-disk0','workhourfile_n-disk1','workhourfile_n-pc0','workhourfile_n-pc1','workhourfile_n-pc2','workhourfile_n-pc3','workhourfile_n_compf','workhourfile_compf_mean_file_len','workhourfile_compf_mean_file_depth','workhourfile_compf_mean_file_nwords','workhourfile_compf_n-disk0','workhourfile_compf_n-disk1','workhourfile_compf_n-pc0','workhourfile_compf_n-pc1','workhourfile_compf_n-pc2','workhourfile_compf_n-pc3','workhourfile_n_phof','workhourfile_phof_mean_file_len','workhourfile_phof_mean_file_depth','workhourfile_phof_mean_file_nwords','workhourfile_phof_n-disk0','workhourfile_phof_n-disk1','workhourfile_phof_n-pc0','workhourfile_phof_n-pc1','workhourfile_phof_n-pc2','workhourfile_phof_n-pc3','workhourfile_n_docf','workhourfile_docf_mean_file_len','workhourfile_docf_mean_file_depth','workhourfile_docf_mean_file_nwords','workhourfile_docf_n-disk0','workhourfile_docf_n-disk1','workhourfile_docf_n-pc0','workhourfile_docf_n-pc1','workhourfile_docf_n-pc2','workhourfile_docf_n-pc3','workhourfile_n_txtf','workhourfile_txtf_mean_file_len','workhourfile_txtf_mean_file_depth','workhourfile_txtf_mean_file_nwords','workhourfile_txtf_n-disk0','workhourfile_txtf_n-disk1','workhourfile_txtf_n-pc0','workhourfile_txtf_n-pc1','workhourfile_txtf_n-pc2','workhourfile_txtf_n-pc3','workhourfile_n_exef','workhourfile_exef_mean_file_len','workhourfile_exef_mean_file_depth','workhourfile_exef_mean_file_nwords','workhourfile_exef_n-disk0','workhourfile_exef_n-disk1','workhourfile_exef_n-pc0','workhourfile_exef_n-pc1','workhourfile_exef_n-pc2','workhourfile_exef_n-pc3','n_afterhourfile','afterhourfile_mean_file_len','afterhourfile_mean_file_depth','afterhourfile_mean_file_nwords','afterhourfile_n-disk0','afterhourfile_n-disk1','afterhourfile_n-pc0','afterhourfile_n-pc1','afterhourfile_n-pc2','afterhourfile_n-pc3','afterhourfile_n_compf','afterhourfile_compf_mean_file_len','afterhourfile_compf_mean_file_depth','afterhourfile_compf_mean_file_nwords','afterhourfile_compf_n-disk0','afterhourfile_compf_n-disk1','afterhourfile_compf_n-pc0','afterhourfile_compf_n-pc1','afterhourfile_compf_n-pc2','afterhourfile_compf_n-pc3','afterhourfile_n_phof','afterhourfile_phof_mean_file_len','afterhourfile_phof_mean_file_depth','afterhourfile_phof_mean_file_nwords','afterhourfile_phof_n-disk0','afterhourfile_phof_n-disk1','afterhourfile_phof_n-pc0','afterhourfile_phof_n-pc1','afterhourfile_phof_n-pc2','afterhourfile_phof_n-pc3','afterhourfile_n_docf','afterhourfile_docf_mean_file_len','afterhourfile_docf_mean_file_depth','afterhourfile_docf_mean_file_nwords','afterhourfile_docf_n-disk0','afterhourfile_docf_n-disk1','afterhourfile_docf_n-pc0','afterhourfile_docf_n-pc1','afterhourfile_docf_n-pc2','afterhourfile_docf_n-pc3','afterhourfile_n_txtf','afterhourfile_txtf_mean_file_len','afterhourfile_txtf_mean_file_depth','afterhourfile_txtf_mean_file_nwords','afterhourfile_txtf_n-disk0','afterhourfile_txtf_n-disk1','afterhourfile_txtf_n-pc0','afterhourfile_txtf_n-pc1','afterhourfile_txtf_n-pc2','afterhourfile_txtf_n-pc3','afterhourfile_n_exef','afterhourfile_exef_mean_file_len','afterhourfile_exef_mean_file_depth','afterhourfile_exef_mean_file_nwords','afterhourfile_exef_n-disk0','afterhourfile_exef_n-disk1','afterhourfile_exef_n-pc0','afterhourfile_exef_n-pc1','afterhourfile_exef_n-pc2','afterhourfile_exef_n-pc3','n_email','email_mean_n_des','email_mean_n_atts','email_mean_n_exdes','email_mean_n_bccdes','email_mean_email_size','email_mean_email_text_slen','email_mean_email_text_nwords','email_n-Xemail1','email_n-exbccmail1','email_n-pc0','email_n-pc1','email_n-pc2','email_n-pc3','n_workhouremail','workhouremail_mean_n_des','workhouremail_mean_n_atts','workhouremail_mean_n_exdes','workhouremail_mean_n_bccdes','workhouremail_mean_email_size','workhouremail_mean_email_text_slen','workhouremail_mean_email_text_nwords','workhouremail_n-Xemail1','workhouremail_n-exbccmail1','workhouremail_n-pc0','workhouremail_n-pc1','workhouremail_n-pc2','workhouremail_n-pc3','n_afterhouremail','afterhouremail_mean_n_des','afterhouremail_mean_n_atts','afterhouremail_mean_n_exdes','afterhouremail_mean_n_bccdes','afterhouremail_mean_email_size','afterhouremail_mean_email_text_slen','afterhouremail_mean_email_text_nwords','afterhouremail_n-Xemail1','afterhouremail_n-exbccmail1','afterhouremail_n-pc0','afterhouremail_n-pc1','afterhouremail_n-pc2','afterhouremail_n-pc3','n_http','http_mean_url_len','http_mean_url_depth','http_mean_http_c_len','http_mean_http_c_nwords','http_n-pc0','http_n-pc1','http_n-pc2','http_n-pc3','http_n_otherf','http_otherf_mean_url_len','http_otherf_mean_url_depth','http_otherf_mean_http_c_len','http_otherf_mean_http_c_nwords','http_otherf_n-pc0','http_otherf_n-pc1','http_otherf_n-pc2','http_otherf_n-pc3','http_n_socnetf','http_socnetf_mean_url_len','http_socnetf_mean_url_depth','http_socnetf_mean_http_c_len','http_socnetf_mean_http_c_nwords','http_socnetf_n-pc0','http_socnetf_n-pc1','http_socnetf_n-pc2','http_socnetf_n-pc3','http_n_cloudf','http_cloudf_mean_url_len','http_cloudf_mean_url_depth','http_cloudf_mean_http_c_len','http_cloudf_mean_http_c_nwords','http_cloudf_n-pc0','http_cloudf_n-pc1','http_cloudf_n-pc2','http_cloudf_n-pc3','http_n_jobf','http_jobf_mean_url_len','http_jobf_mean_url_depth','http_jobf_mean_http_c_len','http_jobf_mean_http_c_nwords','http_jobf_n-pc0','http_jobf_n-pc1','http_jobf_n-pc2','http_jobf_n-pc3','http_n_leakf','http_leakf_mean_url_len','http_leakf_mean_url_depth','http_leakf_mean_http_c_len','http_leakf_mean_http_c_nwords','http_leakf_n-pc0','http_leakf_n-pc1','http_leakf_n-pc2','http_leakf_n-pc3','http_n_hackf','http_hackf_mean_url_len','http_hackf_mean_url_depth','http_hackf_mean_http_c_len','http_hackf_mean_http_c_nwords','http_hackf_n-pc0','http_hackf_n-pc1','http_hackf_n-pc2','http_hackf_n-pc3','n_workhourhttp','workhourhttp_mean_url_len','workhourhttp_mean_url_depth','workhourhttp_mean_http_c_len','workhourhttp_mean_http_c_nwords','workhourhttp_n-pc0','workhourhttp_n-pc1','workhourhttp_n-pc2','workhourhttp_n-pc3','workhourhttp_n_otherf','workhourhttp_otherf_mean_url_len','workhourhttp_otherf_mean_url_depth','workhourhttp_otherf_mean_http_c_len','workhourhttp_otherf_mean_http_c_nwords','workhourhttp_otherf_n-pc0','workhourhttp_otherf_n-pc1','workhourhttp_otherf_n-pc2','workhourhttp_otherf_n-pc3','workhourhttp_n_socnetf','workhourhttp_socnetf_mean_url_len','workhourhttp_socnetf_mean_url_depth','workhourhttp_socnetf_mean_http_c_len','workhourhttp_socnetf_mean_http_c_nwords','workhourhttp_socnetf_n-pc0','workhourhttp_socnetf_n-pc1','workhourhttp_socnetf_n-pc2','workhourhttp_socnetf_n-pc3','workhourhttp_n_cloudf','workhourhttp_cloudf_mean_url_len','workhourhttp_cloudf_mean_url_depth','workhourhttp_cloudf_mean_http_c_len','workhourhttp_cloudf_mean_http_c_nwords','workhourhttp_cloudf_n-pc0','workhourhttp_cloudf_n-pc1','workhourhttp_cloudf_n-pc2','workhourhttp_cloudf_n-pc3','workhourhttp_n_jobf','workhourhttp_jobf_mean_url_len','workhourhttp_jobf_mean_url_depth','workhourhttp_jobf_mean_http_c_len','workhourhttp_jobf_mean_http_c_nwords','workhourhttp_jobf_n-pc0','workhourhttp_jobf_n-pc1','workhourhttp_jobf_n-pc2','workhourhttp_jobf_n-pc3','workhourhttp_n_leakf','workhourhttp_leakf_mean_url_len','workhourhttp_leakf_mean_url_depth','workhourhttp_leakf_mean_http_c_len','workhourhttp_leakf_mean_http_c_nwords','workhourhttp_leakf_n-pc0','workhourhttp_leakf_n-pc1','workhourhttp_leakf_n-pc2','workhourhttp_leakf_n-pc3','workhourhttp_n_hackf','workhourhttp_hackf_mean_url_len','workhourhttp_hackf_mean_url_depth','workhourhttp_hackf_mean_http_c_len','workhourhttp_hackf_mean_http_c_nwords','workhourhttp_hackf_n-pc0','workhourhttp_hackf_n-pc1','workhourhttp_hackf_n-pc2','workhourhttp_hackf_n-pc3','n_afterhourhttp','afterhourhttp_mean_url_len','afterhourhttp_mean_url_depth','afterhourhttp_mean_http_c_len','afterhourhttp_mean_http_c_nwords','afterhourhttp_n-pc0','afterhourhttp_n-pc1','afterhourhttp_n-pc2','afterhourhttp_n-pc3','afterhourhttp_n_otherf','afterhourhttp_otherf_mean_url_len','afterhourhttp_otherf_mean_url_depth','afterhourhttp_otherf_mean_http_c_len','afterhourhttp_otherf_mean_http_c_nwords','afterhourhttp_otherf_n-pc0','afterhourhttp_otherf_n-pc1','afterhourhttp_otherf_n-pc2','afterhourhttp_otherf_n-pc3','afterhourhttp_n_socnetf','afterhourhttp_socnetf_mean_url_len','afterhourhttp_socnetf_mean_url_depth','afterhourhttp_socnetf_mean_http_c_len','afterhourhttp_socnetf_mean_http_c_nwords','afterhourhttp_socnetf_n-pc0','afterhourhttp_socnetf_n-pc1','afterhourhttp_socnetf_n-pc2','afterhourhttp_socnetf_n-pc3','afterhourhttp_n_cloudf','afterhourhttp_cloudf_mean_url_len','afterhourhttp_cloudf_mean_url_depth','afterhourhttp_cloudf_mean_http_c_len','afterhourhttp_cloudf_mean_http_c_nwords','afterhourhttp_cloudf_n-pc0','afterhourhttp_cloudf_n-pc1','afterhourhttp_cloudf_n-pc2','afterhourhttp_cloudf_n-pc3','afterhourhttp_n_jobf','afterhourhttp_jobf_mean_url_len','afterhourhttp_jobf_mean_url_depth','afterhourhttp_jobf_mean_http_c_len','afterhourhttp_jobf_mean_http_c_nwords','afterhourhttp_jobf_n-pc0','afterhourhttp_jobf_n-pc1','afterhourhttp_jobf_n-pc2','afterhourhttp_jobf_n-pc3','afterhourhttp_n_leakf','afterhourhttp_leakf_mean_url_len','afterhourhttp_leakf_mean_url_depth','afterhourhttp_leakf_mean_http_c_len','afterhourhttp_leakf_mean_http_c_nwords','afterhourhttp_leakf_n-pc0','afterhourhttp_leakf_n-pc1','afterhourhttp_leakf_n-pc2','afterhourhttp_leakf_n-pc3','afterhourhttp_n_hackf','afterhourhttp_hackf_mean_url_len','afterhourhttp_hackf_mean_url_depth','afterhourhttp_hackf_mean_http_c_len','afterhourhttp_hackf_mean_http_c_nwords','afterhourhttp_hackf_n-pc0','afterhourhttp_hackf_n-pc1','afterhourhttp_hackf_n-pc2','afterhourhttp_hackf_n-pc3'\n",
    "]\n",
    "features_from_chap = ['l1', 'l2', 'l3', 'l4', 'l7', 'l8', 'l9']\n",
    "features = features_from_chap + features_from_intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_features = []\n",
    "for j in features:\n",
    "    mean_features.append('rolling_mean_' + j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.read_pickle('importance.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.dropna(subset=['rolling_mean_afterhourhttp_hackf_n-pc3'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(importance_df, test_size=0.3, random_state=42)\n",
    "\n",
    "for f in features:\n",
    "    scaler = StandardScaler()\n",
    "    train[f] = scaler.fit_transform(np.array(train[f]).reshape(-1,1))\n",
    "    test[f] = scaler.transform(np.array(test[f]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBClassifier(objective='binary:logistic', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_to_keep = 50\n",
    "rfe = RFE(model, n_features_to_select=num_features_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.fit(train[mean_features], train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = np.array(range(len(rfe.support_)))[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_xgb = []\n",
    "\n",
    "for i in selected_features:\n",
    "    print(mean_features[i])\n",
    "    best_features_xgb.append(mean_features[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rolling_mean_l1\n",
    "rolling_mean_l2\n",
    "rolling_mean_l8\n",
    "rolling_mean_n_allact\n",
    "rolling_mean_allact_n-pc0\n",
    "rolling_mean_allact_n-pc1\n",
    "rolling_mean_workhourallact_n-pc0\n",
    "rolling_mean_n_afterhourallact\n",
    "rolling_mean_n_logon\n",
    "rolling_mean_n_workhourlogon\n",
    "rolling_mean_workhourlogon_n-pc0\n",
    "rolling_mean_n_usb\n",
    "rolling_mean_usb_n-pc0\n",
    "rolling_mean_usb_n-pc3\n",
    "rolling_mean_n_workhourusb\n",
    "rolling_mean_workhourusb_n-pc0\n",
    "rolling_mean_afterhourusb_n-pc0\n",
    "rolling_mean_n_file\n",
    "rolling_mean_file_n-disk0\n",
    "rolling_mean_file_n-pc0\n",
    "rolling_mean_file_n_compf\n",
    "rolling_mean_file_compf_mean_file_nwords\n",
    "rolling_mean_file_phof_n-disk0\n",
    "rolling_mean_file_txtf_mean_file_nwords\n",
    "rolling_mean_workhourfile_n-disk1\n",
    "rolling_mean_workhourfile_n_docf\n",
    "rolling_mean_workhourfile_docf_n-disk0\n",
    "rolling_mean_afterhourfile_phof_mean_file_nwords\n",
    "rolling_mean_afterhourfile_phof_n-pc2\n",
    "rolling_mean_email_mean_n_atts\n",
    "rolling_mean_email_n-Xemail1\n",
    "rolling_mean_email_n-exbccmail1\n",
    "rolling_mean_workhouremail_mean_n_atts\n",
    "rolling_mean_afterhouremail_mean_n_atts\n",
    "rolling_mean_afterhouremail_mean_email_size\n",
    "rolling_mean_afterhouremail_n-exbccmail1\n",
    "rolling_mean_n_http\n",
    "rolling_mean_http_mean_url_depth\n",
    "rolling_mean_http_n_otherf\n",
    "rolling_mean_http_otherf_mean_url_len\n",
    "rolling_mean_http_otherf_mean_url_depth\n",
    "rolling_mean_http_jobf_mean_http_c_len\n",
    "rolling_mean_http_leakf_n-pc0\n",
    "rolling_mean_http_n_hackf\n",
    "rolling_mean_n_workhourhttp\n",
    "rolling_mean_workhourhttp_mean_url_depth\n",
    "rolling_mean_workhourhttp_n_otherf\n",
    "rolling_mean_afterhourhttp_cloudf_mean_http_c_len\n",
    "rolling_mean_afterhourhttp_jobf_mean_http_c_len\n",
    "rolling_mean_afterhourhttp_leakf_n-pc0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected = train[best_features_xgb]\n",
    "X_test_selected = test[best_features_xgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_selected, train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model performance\n",
    "accuracy = accuracy_score(test['label'], y_pred)\n",
    "print(f\"Accuracy on the test set: {accuracy:.5f}\")\n",
    "# Evaluate the model performance\n",
    "prec = precision_score(test['label'], y_pred)\n",
    "print(f\"Precision on the test set: {prec:.5f}\")\n",
    "# Evaluate the model performance\n",
    "rec = recall_score(test['label'], y_pred)\n",
    "print(f\"Recall on the test set: {rec:.5f}\")\n",
    "cm = confusion_matrix(test['label'], y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [.2, .3, .4],\n",
    "    'max_depth': [2,4,6,8]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='recall')\n",
    "grid_search.fit(X_train_selected, train['label'])\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBClassifier(objective='binary:logistic', random_state=42, learning_rate= 0.4, max_depth=2, n_estimators= 300)\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "acc_evaluation_results = []\n",
    "rec_evaluation_results = []\n",
    "prec_evaluation_results = []\n",
    "f1_evaluation_results = []\n",
    "\n",
    "for train_index, val_index in kf.split(importance_df):\n",
    "\n",
    "    X_train, X_val = importance_df[best_features_xgb].iloc[train_index], importance_df[best_features_xgb].iloc[val_index]\n",
    "    y_train, y_val = importance_df['label'].iloc[train_index], importance_df['label'].iloc[val_index]\n",
    "\n",
    "    #Standardize\n",
    "    for f in best_features_xgb:\n",
    "        scaler = StandardScaler()\n",
    "        X_train[f] = scaler.fit_transform(np.array(X_train[f]).reshape(-1,1))\n",
    "        X_val[f] = scaler.transform(np.array(X_val[f]).reshape(-1,1))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Evaluate the model performance\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    acc_evaluation_results.append(accuracy)\n",
    "    rec_evaluation_results.append(recall)\n",
    "    prec_evaluation_results.append(precision)\n",
    "    f1_evaluation_results.append(f1)\n",
    "\n",
    "# Print or analyze the evaluation results across folds\n",
    "average_accuracy = np.mean(acc_evaluation_results)\n",
    "print(f'Average Accuracy: {average_accuracy}')\n",
    "\n",
    "average_recall = np.mean(rec_evaluation_results)\n",
    "print(f'Average Recall: {average_recall}')\n",
    "\n",
    "average_precision = np.mean(prec_evaluation_results)\n",
    "print(f'Average Precision: {average_precision}')\n",
    "\n",
    "average_f1 = np.mean(f1_evaluation_results)\n",
    "print(f'Average F1: {average_f1}')\n",
    "\n",
    "cf_matrix = confusion_matrix(y_val, y_pred)\n",
    "print(cf_matrix)\n",
    "\n",
    "sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indActivity = []\n",
    "for index, row in df_final.iterrows():\n",
    "    best_array = []\n",
    "    for f in best_features:\n",
    "        best_array.append(row[f])\n",
    "    indActivity.append(best_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['activityList'] = indActivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROLE DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import datetime\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score as shs\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial.distance import chebyshev\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold, train_test_split, TimeSeriesSplit\n",
    "import shap\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('to_role_measurement.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features =['l1',\n",
    "'l2',\n",
    "'l8',\n",
    "'n_allact',\n",
    "'allact_n-pc0',\n",
    "'allact_n-pc1',\n",
    "'workhourallact_n-pc0',\n",
    "'n_afterhourallact',\n",
    "'n_logon',\n",
    "'n_workhourlogon',\n",
    "'workhourlogon_n-pc0',\n",
    "'n_usb',\n",
    "'usb_n-pc0',\n",
    "'usb_n-pc3',\n",
    "'n_workhourusb',\n",
    "'workhourusb_n-pc0',\n",
    "'afterhourusb_n-pc0',\n",
    "'n_file',\n",
    "'file_n-disk0',\n",
    "'file_n-pc0',\n",
    "'file_n_compf',\n",
    "'file_compf_mean_file_nwords',\n",
    "'file_phof_n-disk0',\n",
    "'file_txtf_mean_file_nwords',\n",
    "'workhourfile_n-disk1',\n",
    "'workhourfile_n_docf',\n",
    "'workhourfile_docf_n-disk0',\n",
    "'afterhourfile_phof_mean_file_nwords',\n",
    "'afterhourfile_phof_n-pc2',\n",
    "'email_mean_n_atts',\n",
    "'email_n-Xemail1',\n",
    "'email_n-exbccmail1',\n",
    "'workhouremail_mean_n_atts',\n",
    "'afterhouremail_mean_n_atts',\n",
    "'afterhouremail_mean_email_size',\n",
    "'afterhouremail_n-exbccmail1',\n",
    "'n_http',\n",
    "'http_mean_url_depth',\n",
    "'http_n_otherf',\n",
    "'http_otherf_mean_url_len',\n",
    "'http_otherf_mean_url_depth',\n",
    "'http_jobf_mean_http_c_len',\n",
    "'http_leakf_n-pc0',\n",
    "'http_n_hackf',\n",
    "'n_workhourhttp',\n",
    "'workhourhttp_mean_url_depth',\n",
    "'workhourhttp_n_otherf',\n",
    "'afterhourhttp_cloudf_mean_http_c_len',\n",
    "'afterhourhttp_jobf_mean_http_c_len',\n",
    "'afterhourhttp_leakf_n-pc0',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = datetime.date(2010,1,2)\n",
    "END = datetime.date(2011,12,31)\n",
    "dates = pd.date_range(start=START, end=END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = df['role'].unique()\n",
    "users = df['user'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_features = ['l1',\n",
    "'l2',\n",
    "'l8',\n",
    "'n_logon',\n",
    "'n_workhourlogon',\n",
    "'workhourlogon_n-pc0']\n",
    "\n",
    "activity_features = ['n_allact',\n",
    "'allact_n-pc0',\n",
    "'allact_n-pc1',\n",
    "'workhourallact_n-pc0',\n",
    "'n_afterhourallact']\n",
    "\n",
    "usb_features = ['n_usb',\n",
    "'usb_n-pc0',\n",
    "'usb_n-pc3',\n",
    "'n_workhourusb',\n",
    "'workhourusb_n-pc0',\n",
    "'afterhourusb_n-pc0']\n",
    "\n",
    "file_features = ['n_file',\n",
    "'file_n-disk0',\n",
    "'file_n-pc0',\n",
    "'file_n_compf',\n",
    "'file_compf_mean_file_nwords',\n",
    "'file_phof_n-disk0',\n",
    "'file_txtf_mean_file_nwords',\n",
    "'workhourfile_n-disk1',\n",
    "'workhourfile_n_docf',\n",
    "'workhourfile_docf_n-disk0',\n",
    "'afterhourfile_phof_mean_file_nwords',\n",
    "'afterhourfile_phof_n-pc2',\n",
    "]\n",
    "\n",
    "email_features = ['email_n-Xemail1',\n",
    "'email_n-exbccmail1',\n",
    "'workhouremail_mean_n_atts',\n",
    "'afterhouremail_mean_n_atts',\n",
    "'afterhouremail_mean_email_size',\n",
    "'afterhouremail_n-exbccmail1',\n",
    "'email_mean_n_atts']\n",
    "\n",
    "http_features = ['n_http',\n",
    "'http_mean_url_depth',\n",
    "'http_n_otherf',\n",
    "'http_otherf_mean_url_len',\n",
    "'http_otherf_mean_url_depth',\n",
    "'http_jobf_mean_http_c_len',\n",
    "'http_leakf_n-pc0',\n",
    "'http_n_hackf',\n",
    "'n_workhourhttp',\n",
    "'workhourhttp_mean_url_depth',\n",
    "'workhourhttp_n_otherf',\n",
    "'afterhourhttp_cloudf_mean_http_c_len',\n",
    "'afterhourhttp_jobf_mean_http_c_len',\n",
    "'afterhourhttp_leakf_n-pc0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = pd.DataFrame()\n",
    "\n",
    "roll_period = 14\n",
    "\n",
    "for u in tqdm(users):\n",
    "    df_u = df.loc[df['user'] == u]\n",
    "    df_u.sort_values(by='date', inplace=True, ignore_index=True)\n",
    "    df_u.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    for j in best_features:\n",
    "        df_u['rolling_mean_' + j] = df_u[j].rolling(roll_period).mean()\n",
    "        df_u['rolling_median_' + j] = df_u[j].rolling(roll_period).median()\n",
    "        df_u['rolling_iqr_' + j] = df_u[j].rolling(roll_period).quantile(0.75) - df_u[j].rolling(roll_period).quantile(0.25)\n",
    "        df_u['rolling_std_'+ j] =  df_u[j].rolling(roll_period).std()\n",
    "\n",
    "    reconstructed = pd.concat([reconstructed, df_u]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_features = []\n",
    "median_features = []\n",
    "iqr_features = []\n",
    "std_features = []\n",
    "\n",
    "for j in best_features:\n",
    "    mean_features.append('rolling_mean_' + j)\n",
    "    median_features.append('rolling_median_' + j)\n",
    "    iqr_features.append('rolling_iqr_' + j)\n",
    "    std_features.append('rolling_std_' + j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['date', 'user'], ascending=[True, False])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROLE CENTRALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roleMeanActivityList = []\n",
    "roleMedianActivityList = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = df.loc[(df['date'] == d)]\n",
    "    for r in roles:\n",
    "        roleDateMeanActivityList = []\n",
    "        roleDateMedianActivityList = []\n",
    "        role_df = daysEvents.loc[(daysEvents['role'] == r)]\n",
    "        user_count = role_df.shape[0]\n",
    "        if (user_count >= 1):\n",
    "            for f in best_features:\n",
    "                f_mean = role_df[f].mean()\n",
    "                f_Median = role_df[f].median()\n",
    "                roleDateMeanActivityList.append(f_mean)\n",
    "                roleDateMedianActivityList.append(f_Median)\n",
    "            roleMeanActivityList.append([d, r, roleDateMeanActivityList])\n",
    "            roleMedianActivityList.append([d, r, roleDateMedianActivityList])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roleLoginMeanActivityList = []\n",
    "roleLoginMedianActivityList = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = df.loc[(df['date'] == d)]\n",
    "    for r in roles:\n",
    "        roleLoginDateMeanActivityList = []\n",
    "        roleLoginDateMedianActivityList = []\n",
    "        role_df = daysEvents.loc[(daysEvents['role'] == r)]\n",
    "        user_count = role_df.shape[0]\n",
    "        if (user_count >= 1):\n",
    "            for f in login_features:\n",
    "                f_mean = role_df[f].mean()\n",
    "                f_Median = role_df[f].median()\n",
    "                roleLoginDateMeanActivityList.append(f_mean)\n",
    "                roleLoginDateMedianActivityList.append(f_mean)\n",
    "            roleLoginMeanActivityList.append([d, r, roleLoginDateMeanActivityList])\n",
    "            roleLoginMedianActivityList.append([d, r, roleLoginDateMedianActivityList])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roleEmailMeanActivityList = []\n",
    "roleEmailMedianActivityList = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = df.loc[(df['date'] == d)]\n",
    "    for r in roles:\n",
    "        roleEmailDateMeanActivityList = []\n",
    "        roleEmailDateMedianActivityList = []\n",
    "        role_df = daysEvents.loc[(daysEvents['role'] == r)]\n",
    "        user_count = role_df.shape[0]\n",
    "        if (user_count >= 1):\n",
    "            for f in email_features:\n",
    "                f_mean = role_df[f].mean()\n",
    "                f_Median = role_df[f].median()\n",
    "                roleEmailDateMeanActivityList.append(f_mean)\n",
    "                roleEmailDateMedianActivityList.append(f_mean)\n",
    "            roleEmailMeanActivityList.append([d, r, roleEmailDateMeanActivityList])\n",
    "            roleEmailMedianActivityList.append([d, r, roleEmailDateMedianActivityList])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roleHttpMeanActivityList = []\n",
    "roleHttpMedianActivityList = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = df.loc[(df['date'] == d)]\n",
    "    for r in roles:\n",
    "        roleHttpDateMeanActivityList = []\n",
    "        roleHttpDateMedianActivityList = []\n",
    "        role_df = daysEvents.loc[(daysEvents['role'] == r)]\n",
    "        user_count = role_df.shape[0]\n",
    "        if (user_count >= 1):\n",
    "            for f in http_features:\n",
    "                f_mean = role_df[f].mean()\n",
    "                f_Median = role_df[f].median()\n",
    "                roleHttpDateMeanActivityList.append(f_mean)\n",
    "                roleHttpDateMedianActivityList.append(f_mean)\n",
    "            roleHttpMeanActivityList.append([d, r, roleHttpDateMeanActivityList])\n",
    "            roleHttpMedianActivityList.append([d, r, roleHttpDateMedianActivityList])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roleActivityMeanActivityList = []\n",
    "roleActivityMedianActivityList = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = df.loc[(df['date'] == d)]\n",
    "    for r in roles:\n",
    "        roleActivityDateMeanActivityList = []\n",
    "        roleActivityDateMedianActivityList = []\n",
    "        role_df = daysEvents.loc[(daysEvents['role'] == r)]\n",
    "        user_count = role_df.shape[0]\n",
    "        if (user_count >= 1):\n",
    "            for f in activity_features:\n",
    "                f_mean = role_df[f].mean()\n",
    "                f_Median = role_df[f].median()\n",
    "                roleActivityDateMeanActivityList.append(f_mean)\n",
    "                roleActivityDateMedianActivityList.append(f_mean)\n",
    "            roleActivityMeanActivityList.append([d, r, roleActivityDateMeanActivityList])\n",
    "            roleActivityMedianActivityList.append([d, r, roleActivityDateMedianActivityList])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roleFileMeanActivityList = []\n",
    "roleFileMedianActivityList = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = df.loc[(df['date'] == d)]\n",
    "    for r in roles:\n",
    "        roleFileDateMeanActivityList = []\n",
    "        roleFileDateMedianActivityList = []\n",
    "        role_df = daysEvents.loc[(daysEvents['role'] == r)]\n",
    "        user_count = role_df.shape[0]\n",
    "        if (user_count >= 1):\n",
    "            for f in file_features:\n",
    "                f_mean = role_df[f].mean()\n",
    "                f_Median = role_df[f].median()\n",
    "                roleFileDateMeanActivityList.append(f_mean)\n",
    "                roleFileDateMedianActivityList.append(f_mean)\n",
    "            roleFileMeanActivityList.append([d, r, roleFileDateMeanActivityList])\n",
    "            roleFileMedianActivityList.append([d, r, roleFileDateMedianActivityList])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roleUsbMeanActivityList = []\n",
    "roleUsbMedianActivityList = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = df.loc[(df['date'] == d)]\n",
    "    for r in roles:\n",
    "        roleUsbDateMeanActivityList = []\n",
    "        roleUsbDateMedianActivityList = []\n",
    "        role_df = daysEvents.loc[(daysEvents['role'] == r)]\n",
    "        user_count = role_df.shape[0]\n",
    "        if (user_count >= 1):\n",
    "            for f in usb_features:\n",
    "                f_mean = role_df[f].mean()\n",
    "                f_Median = role_df[f].median()\n",
    "                roleUsbDateMeanActivityList.append(f_mean)\n",
    "                roleUsbDateMedianActivityList.append(f_mean)\n",
    "            roleUsbMeanActivityList.append([d, r, roleUsbDateMeanActivityList])\n",
    "            roleUsbMedianActivityList.append([d, r, roleUsbDateMedianActivityList])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_mean_df = pd.DataFrame(roleMeanActivityList, columns=['date', 'role', 'roleMeanActivityList'])\n",
    "role_Median_df = pd.DataFrame(roleMedianActivityList, columns=['date', 'role', 'roleMedianActivityList'])\n",
    "\n",
    "role_login_mean_df = pd.DataFrame(roleLoginMeanActivityList, columns=['date', 'role', 'roleLoginMeanActivityList'])\n",
    "role_login_Median_df = pd.DataFrame(roleLoginMedianActivityList, columns=['date', 'role', 'roleLoginMedianActivityList'])\n",
    "\n",
    "role_http_mean_df = pd.DataFrame(roleHttpMeanActivityList, columns=['date', 'role', 'roleHttpMeanActivityList'])\n",
    "role_http_Median_df = pd.DataFrame(roleHttpMedianActivityList, columns=['date', 'role', 'roleHttpMedianActivityList'])\n",
    "\n",
    "role_email_mean_df = pd.DataFrame(roleEmailMeanActivityList, columns=['date', 'role', 'roleEmailMeanActivityList'])\n",
    "role_email_Median_df = pd.DataFrame(roleEmailMedianActivityList, columns=['date', 'role', 'roleEmailMedianActivityList'])\n",
    "\n",
    "role_file_mean_df = pd.DataFrame(roleFileMeanActivityList, columns=['date', 'role', 'roleFileMeanActivityList'])\n",
    "role_file_Median_df = pd.DataFrame(roleFileMedianActivityList, columns=['date', 'role', 'roleFileMedianActivityList'])\n",
    "\n",
    "role_usb_mean_df = pd.DataFrame(roleUsbMeanActivityList, columns=['date', 'role', 'roleUsbMeanActivityList'])\n",
    "role_usb_Median_df = pd.DataFrame(roleUsbMedianActivityList, columns=['date', 'role', 'roleUsbMedianActivityList'])\n",
    "\n",
    "role_activity_mean_df = pd.DataFrame(roleActivityMeanActivityList, columns=['date', 'role', 'roleActivityMeanActivityList'])\n",
    "role_activity_Median_df = pd.DataFrame(roleActivityMedianActivityList, columns=['date', 'role', 'roleActivityMedianActivityList'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, role_mean_df, on=['date', 'role'], how='left')\n",
    "df = pd.merge(df, role_login_mean_df, on=['date', 'role'], how='left')\n",
    "df = pd.merge(df, role_http_mean_df, on=['date', 'role'], how='left')\n",
    "df = pd.merge(df, role_usb_mean_df, on=['date', 'role'], how='left')\n",
    "df = pd.merge(df, role_activity_mean_df, on=['date', 'role'], how='left')\n",
    "df = pd.merge(df, role_file_mean_df, on=['date', 'role'], how='left')\n",
    "df = pd.merge(df, role_email_mean_df, on=['date', 'role'], how='left')\n",
    "\n",
    "df = pd.merge(df, role_Median_df, on=['date', 'role'], how='left')\n",
    "df = pd.merge(df, role_login_Median_df, on=['date', 'role'], how='left')\n",
    "df = pd.merge(df, role_http_Median_df, on=['date', 'role'], how='left')\n",
    "df = pd.merge(df, role_usb_Median_df, on=['date', 'role'], how='left')\n",
    "df = pd.merge(df, role_activity_Median_df, on=['date', 'role'], how='left')\n",
    "df = pd.merge(df, role_file_Median_df, on=['date', 'role'], how='left')\n",
    "df = pd.merge(df, role_email_Median_df, on=['date', 'role'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_selected_columns(row, columns_to_combine):\n",
    "    selected_values = row[columns_to_combine].tolist()\n",
    "    return selected_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['activityList'] = df.apply(combine_selected_columns, columns_to_combine=best_features, axis=1)\n",
    "df['loginActivityList'] = df.apply(combine_selected_columns, columns_to_combine=login_features, axis=1)\n",
    "df['httpActivityList'] = df.apply(combine_selected_columns, columns_to_combine=http_features, axis=1)\n",
    "df['emailActivityList'] = df.apply(combine_selected_columns, columns_to_combine=email_features, axis=1)\n",
    "df['usbActivityList'] = df.apply(combine_selected_columns, columns_to_combine=usb_features, axis=1)\n",
    "df['fileActivityList'] = df.apply(combine_selected_columns, columns_to_combine=file_features, axis=1)\n",
    "df['activityActivityList'] = df.apply(combine_selected_columns, columns_to_combine=activity_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dropna(subset = ['rolling_mean_l1'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the distance between the user's activity vector and that of the role the user belongs to.  Cosine Similarity, Euclidiean Distance, and Dot Product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vector(vector):\n",
    "    norm = np.linalg.norm(vector)\n",
    "    if norm == 0:\n",
    "        return vector\n",
    "    return vector / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    roleDist = (np.subtract(np.array(row['activityList']), np.array(row['roleMeanActivityList'])))\n",
    "    A = np.array(row['activityList'])\n",
    "    B = np.array(row['roleMeanActivityList'])\n",
    "    normalized_A = normalize_vector(A)\n",
    "    normalized_B = normalize_vector(B)\n",
    "    cos_sim = cosine_similarity([normalized_A], [normalized_B])\n",
    "    cos_sim = cos_sim[0,0]\n",
    "    euc_dist = euclidean_distances([A], [B])\n",
    "    euc_dist = euc_dist[0,0]\n",
    "    che_dist = chebyshev(A, B)\n",
    "    dot_prod = np.dot(A, B)\n",
    "    user = row['user']\n",
    "    date = row['date']\n",
    "    list_row = {'role_dist':roleDist, 'cos_sim':cos_sim, 'che_dist':che_dist, 'euc_dist':euc_dist, 'dot_prod':dot_prod, 'user':user, 'date':date}\n",
    "    temp.append(list_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame(temp, columns=['role_dist', 'cos_sim', 'che_dist', 'euc_dist', 'dot_prod', 'user', 'date'])\n",
    "df = pd.merge(df, df_temp, on=['date', 'user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    roleDist = (np.subtract(np.array(row['loginActivityList']), np.array(row['roleLoginMeanActivityList'])))\n",
    "    A = np.array(row['loginActivityList'])\n",
    "    B = np.array(row['roleLoginMeanActivityList'])\n",
    "    normalized_A = normalize_vector(A)\n",
    "    normalized_B = normalize_vector(B)\n",
    "    cos_sim = cosine_similarity([normalized_A], [normalized_B])\n",
    "    cos_sim = cos_sim[0,0]\n",
    "    euc_dist = euclidean_distances([A], [B])\n",
    "    euc_dist = euc_dist[0,0]\n",
    "    che_dist = chebyshev(A, B)\n",
    "    dot_prod = np.dot(A, B)\n",
    "    user = row['user']\n",
    "    date = row['date']\n",
    "    list_row = {'role_dist_login':roleDist, 'cos_sim_login':cos_sim, 'che_dist_login':che_dist, 'euc_dist_login':euc_dist, 'dot_prod_login':dot_prod, 'user':user, 'date':date}\n",
    "    temp.append(list_row)\n",
    "df_temp = pd.DataFrame(temp, columns=['role_dist_login', 'cos_sim_login', 'che_dist_login', 'euc_dist_login', 'dot_prod_login', 'user', 'date'])\n",
    "df = pd.merge(df, df_temp, on=['date', 'user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    roleDist = (np.subtract(np.array(row['httpActivityList']), np.array(row['roleHttpMeanActivityList'])))\n",
    "    A = np.array(row['httpActivityList'])\n",
    "    B = np.array(row['roleHttpMeanActivityList'])\n",
    "    normalized_A = normalize_vector(A)\n",
    "    normalized_B = normalize_vector(B)\n",
    "    cos_sim = cosine_similarity([normalized_A], [normalized_B])\n",
    "    cos_sim = cos_sim[0,0]\n",
    "    euc_dist = euclidean_distances([A], [B])\n",
    "    euc_dist = euc_dist[0,0]\n",
    "    che_dist = chebyshev(A, B)\n",
    "    dot_prod = np.dot(A, B)\n",
    "    user = row['user']\n",
    "    date = row['date']\n",
    "    list_row = {'role_dist_http':roleDist, 'cos_sim_http':cos_sim, 'che_dist_http':che_dist, 'euc_dist_http':euc_dist, 'dot_prod_http':dot_prod, 'user':user, 'date':date}\n",
    "    temp.append(list_row)\n",
    "df_temp = pd.DataFrame(temp, columns=['role_dist_http', 'cos_sim_http', 'che_dist_http', 'euc_dist_http', 'dot_prod_http', 'user', 'date'])\n",
    "df = pd.merge(df, df_temp, on=['date', 'user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    roleDist = (np.subtract(np.array(row['emailActivityList']), np.array(row['roleEmailMeanActivityList'])))\n",
    "    A = np.array(row['emailActivityList'])\n",
    "    B = np.array(row['roleEmailMeanActivityList'])\n",
    "    normalized_A = normalize_vector(A)\n",
    "    normalized_B = normalize_vector(B)\n",
    "    cos_sim = cosine_similarity([normalized_A], [normalized_B])\n",
    "    cos_sim = cos_sim[0,0]\n",
    "    euc_dist = euclidean_distances([A], [B])\n",
    "    euc_dist = euc_dist[0,0]\n",
    "    che_dist = chebyshev(A, B)\n",
    "    dot_prod = np.dot(A, B)\n",
    "    user = row['user']\n",
    "    date = row['date']\n",
    "    list_row = {'role_dist_email':roleDist, 'cos_sim_email':cos_sim, 'che_dist_email':che_dist, 'euc_dist_email':euc_dist, 'dot_prod_email':dot_prod, 'user':user, 'date':date}\n",
    "    temp.append(list_row)\n",
    "df_temp = pd.DataFrame(temp, columns=['role_dist_email', 'cos_sim_email', 'che_dist_email', 'euc_dist_email', 'dot_prod_email', 'user', 'date'])\n",
    "df = pd.merge(df, df_temp, on=['date', 'user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    roleDist = (np.subtract(np.array(row['fileActivityList']), np.array(row['roleFileMeanActivityList'])))\n",
    "    A = np.array(row['fileActivityList'])\n",
    "    B = np.array(row['roleFileMeanActivityList'])\n",
    "    normalized_A = normalize_vector(A)\n",
    "    normalized_B = normalize_vector(B)\n",
    "    cos_sim = cosine_similarity([normalized_A], [normalized_B])\n",
    "    cos_sim = cos_sim[0,0]\n",
    "    euc_dist = euclidean_distances([A], [B])\n",
    "    euc_dist = euc_dist[0,0]\n",
    "    che_dist = chebyshev(A, B)\n",
    "    dot_prod = np.dot(A, B)\n",
    "    user = row['user']\n",
    "    date = row['date']\n",
    "    list_row = {'role_dist_file':roleDist, 'cos_sim_file':cos_sim, 'che_dist_file':che_dist, 'euc_dist_file':euc_dist, 'dot_prod_file':dot_prod, 'user':user, 'date':date}\n",
    "    temp.append(list_row)\n",
    "df_temp = pd.DataFrame(temp, columns=['role_dist_file', 'cos_sim_file', 'che_dist_file', 'euc_dist_file', 'dot_prod_file', 'user', 'date'])\n",
    "\n",
    "df = pd.merge(df, df_temp, on=['date', 'user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    roleDist = (np.subtract(np.array(row['usbActivityList']), np.array(row['roleUsbMeanActivityList'])))\n",
    "    A = np.array(row['usbActivityList'])\n",
    "    B = np.array(row['roleUsbMeanActivityList'])\n",
    "    normalized_A = normalize_vector(A)\n",
    "    normalized_B = normalize_vector(B)\n",
    "    cos_sim = cosine_similarity([normalized_A], [normalized_B])\n",
    "    cos_sim = cos_sim[0,0]\n",
    "    euc_dist = euclidean_distances([A], [B])\n",
    "    euc_dist = euc_dist[0,0]\n",
    "    che_dist = chebyshev(A, B)\n",
    "    dot_prod = np.dot(A, B)\n",
    "    user = row['user']\n",
    "    date = row['date']\n",
    "    list_row = {'role_dist_usb':roleDist, 'cos_sim_usb':cos_sim, 'che_dist_usb':che_dist, 'euc_dist_usb':euc_dist, 'dot_prod_usb':dot_prod, 'user':user, 'date':date}\n",
    "    temp.append(list_row)\n",
    "df_temp = pd.DataFrame(temp, columns=['role_dist_usb', 'cos_sim_usb', 'che_dist_usb', 'euc_dist_usb', 'dot_prod_usb', 'user', 'date'])\n",
    "\n",
    "df = pd.merge(df, df_temp, on=['date', 'user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    roleDist = (np.subtract(np.array(row['activityActivityList']), np.array(row['roleActivityMeanActivityList'])))\n",
    "    A = np.array(row['activityActivityList'])\n",
    "    B = np.array(row['roleActivityMeanActivityList'])\n",
    "    normalized_A = normalize_vector(A)\n",
    "    normalized_B = normalize_vector(B)\n",
    "    cos_sim = cosine_similarity([normalized_A], [normalized_B])\n",
    "    cos_sim = cos_sim[0,0]\n",
    "    euc_dist = euclidean_distances([A], [B])\n",
    "    euc_dist = euc_dist[0,0]\n",
    "    che_dist = chebyshev(A, B)\n",
    "    dot_prod = np.dot(A, B)\n",
    "    user = row['user']\n",
    "    date = row['date']\n",
    "    list_row = {'role_dist_act':roleDist, 'cos_sim_act':cos_sim, 'che_dist_act':che_dist, 'euc_dist_act':euc_dist, 'dot_prod_act':dot_prod, 'user':user, 'date':date}\n",
    "    temp.append(list_row)\n",
    "df_temp = pd.DataFrame(temp, columns=['role_dist_act', 'cos_sim_act', 'che_dist_act', 'euc_dist_act', 'dot_prod_act', 'user', 'date'])\n",
    "df = pd.merge(df, df_temp, on=['date', 'user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('to_psy_measurement.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Psy Distance Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as the role distance section, but now the Pschometric classification is used to measure the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldap.rename(columns={'user_id': 'user'}, inplace=True)\n",
    "df = pd.merge(df, ldap, on=['user'], how='left')\n",
    "clusters = df['cluster'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psyMeanActivityList = []\n",
    "psyMedianActivityList = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = df.loc[(df['date'] == d)]\n",
    "    for c in clusters:\n",
    "        psyDateMeanActivityList = []\n",
    "        psyDateMedianActivityList = []\n",
    "        psy_df = daysEvents.loc[(daysEvents['cluster'] == c)]\n",
    "        user_count = psy_df.shape[0]\n",
    "        if (user_count >= 1):\n",
    "            for f in best_features:\n",
    "                f_mean = psy_df[f].mean()\n",
    "                f_Median = psy_df[f].median()\n",
    "                psyDateMeanActivityList.append(f_mean)\n",
    "                psyDateMedianActivityList.append(f_Median)\n",
    "            psyMeanActivityList.append([d, c, psyDateMeanActivityList])\n",
    "            psyMedianActivityList.append([d, c, psyDateMedianActivityList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psyLoginMeanActivityList = []\n",
    "psyLoginMedianActivityList = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = df.loc[(df['date'] == d)]\n",
    "    for c in clusters:\n",
    "        psyLoginDateMeanActivityList = []\n",
    "        psyLoginDateMedianActivityList = []\n",
    "        psy_df = daysEvents.loc[(daysEvents['cluster'] == c)]\n",
    "        user_count = psy_df.shape[0]\n",
    "        if (user_count >= 1):\n",
    "            for f in login_features:\n",
    "                f_mean = psy_df[f].mean()\n",
    "                f_Median = psy_df[f].median()\n",
    "                psyLoginDateMeanActivityList.append(f_mean)\n",
    "                psyLoginDateMedianActivityList.append(f_mean)\n",
    "            psyLoginMeanActivityList.append([d, c, psyLoginDateMeanActivityList])\n",
    "            psyLoginMedianActivityList.append([d, c, psyLoginDateMedianActivityList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psyEmailMeanActivityList = []\n",
    "psyEmailMedianActivityList = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = df.loc[(df['date'] == d)]\n",
    "    for c in clusters:\n",
    "        psyEmailDateMeanActivityList = []\n",
    "        psyEmailDateMedianActivityList = []\n",
    "        psy_df = daysEvents.loc[(daysEvents['cluster'] == c)]\n",
    "        user_count = psy_df.shape[0]\n",
    "        if (user_count >= 1):\n",
    "            for f in email_features:\n",
    "                f_mean = psy_df[f].mean()\n",
    "                f_Median = psy_df[f].median()\n",
    "                psyEmailDateMeanActivityList.append(f_mean)\n",
    "                psyEmailDateMedianActivityList.append(f_mean)\n",
    "            psyEmailMeanActivityList.append([d, c, psyEmailDateMeanActivityList])\n",
    "            psyEmailMedianActivityList.append([d, c, psyEmailDateMedianActivityList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psyHttpMeanActivityList = []\n",
    "psyHttpMedianActivityList = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = df.loc[(df['date'] == d)]\n",
    "    for c in clusters:\n",
    "        psyHttpDateMeanActivityList = []\n",
    "        psyHttpDateMedianActivityList = []\n",
    "        psy_df = daysEvents.loc[(daysEvents['cluster'] == c)]\n",
    "        user_count = psy_df.shape[0]\n",
    "        if (user_count >= 1):\n",
    "            for f in http_features:\n",
    "                f_mean = psy_df[f].mean()\n",
    "                f_Median = psy_df[f].median()\n",
    "                psyHttpDateMeanActivityList.append(f_mean)\n",
    "                psyHttpDateMedianActivityList.append(f_mean)\n",
    "            psyHttpMeanActivityList.append([d, c, psyHttpDateMeanActivityList])\n",
    "            psyHttpMedianActivityList.append([d, c, psyHttpDateMedianActivityList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psyActivityMeanActivityList = []\n",
    "psyActivityMedianActivityList = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = df.loc[(df['date'] == d)]\n",
    "    for c in clusters:\n",
    "        psyActivityDateMeanActivityList = []\n",
    "        psyActivityDateMedianActivityList = []\n",
    "        psy_df = daysEvents.loc[(daysEvents['cluster'] == c)]\n",
    "        user_count = psy_df.shape[0]\n",
    "        if (user_count >= 1):\n",
    "            for f in activity_features:\n",
    "                f_mean = psy_df[f].mean()\n",
    "                f_Median = psy_df[f].median()\n",
    "                psyActivityDateMeanActivityList.append(f_mean)\n",
    "                psyActivityDateMedianActivityList.append(f_mean)\n",
    "            psyActivityMeanActivityList.append([d, c, psyActivityDateMeanActivityList])\n",
    "            psyActivityMedianActivityList.append([d, c, psyActivityDateMedianActivityList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psyFileMeanActivityList = []\n",
    "psyFileMedianActivityList = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = df.loc[(df['date'] == d)]\n",
    "    for c in clusters:\n",
    "        psyFileDateMeanActivityList = []\n",
    "        psyFileDateMedianActivityList = []\n",
    "        psy_df = daysEvents.loc[(daysEvents['cluster'] == c)]\n",
    "        user_count = psy_df.shape[0]\n",
    "        if (user_count >= 1):\n",
    "            for f in file_features:\n",
    "                f_mean = psy_df[f].mean()\n",
    "                f_Median = psy_df[f].median()\n",
    "                psyFileDateMeanActivityList.append(f_mean)\n",
    "                psyFileDateMedianActivityList.append(f_mean)\n",
    "            psyFileMeanActivityList.append([d, c, psyFileDateMeanActivityList])\n",
    "            psyFileMedianActivityList.append([d, c, psyFileDateMedianActivityList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psyUsbMeanActivityList = []\n",
    "psyUsbMedianActivityList = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    daysEvents = df.loc[(df['date'] == d)]\n",
    "    for c in clusters:\n",
    "        psyUsbDateMeanActivityList = []\n",
    "        psyUsbDateMedianActivityList = []\n",
    "        psy_df = daysEvents.loc[(daysEvents['cluster'] == c)]\n",
    "        user_count = psy_df.shape[0]\n",
    "        if (user_count >= 1):\n",
    "            for f in usb_features:\n",
    "                f_mean = psy_df[f].mean()\n",
    "                f_Median = psy_df[f].median()\n",
    "                psyUsbDateMeanActivityList.append(f_mean)\n",
    "                psyUsbDateMedianActivityList.append(f_mean)\n",
    "            psyUsbMeanActivityList.append([d, c, psyUsbDateMeanActivityList])\n",
    "            psyUsbMedianActivityList.append([d, c, psyUsbDateMedianActivityList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psy_mean_df = pd.DataFrame(psyMeanActivityList, columns=['date', 'cluster', 'psyMeanActivityList'])\n",
    "psy_Median_df = pd.DataFrame(psyMedianActivityList, columns=['date', 'cluster', 'psyMedianActivityList'])\n",
    "\n",
    "psy_login_mean_df = pd.DataFrame(psyLoginMeanActivityList, columns=['date', 'cluster', 'psyLoginMeanActivityList'])\n",
    "psy_login_Median_df = pd.DataFrame(psyLoginMedianActivityList, columns=['date', 'cluster', 'psyLoginMedianActivityList'])\n",
    "\n",
    "psy_http_mean_df = pd.DataFrame(psyHttpMeanActivityList, columns=['date', 'cluster', 'psyHttpMeanActivityList'])\n",
    "psy_http_Median_df = pd.DataFrame(psyHttpMedianActivityList, columns=['date', 'cluster', 'psyHttpMedianActivityList'])\n",
    "\n",
    "psy_email_mean_df = pd.DataFrame(psyEmailMeanActivityList, columns=['date', 'cluster', 'psyEmailMeanActivityList'])\n",
    "psy_email_Median_df = pd.DataFrame(psyEmailMedianActivityList, columns=['date', 'cluster', 'psyEmailMedianActivityList'])\n",
    "\n",
    "psy_file_mean_df = pd.DataFrame(psyFileMeanActivityList, columns=['date', 'cluster', 'psyFileMeanActivityList'])\n",
    "psy_file_Median_df = pd.DataFrame(psyFileMedianActivityList, columns=['date', 'cluster', 'psyFileMedianActivityList'])\n",
    "\n",
    "psy_usb_mean_df = pd.DataFrame(psyUsbMeanActivityList, columns=['date', 'cluster', 'psyUsbMeanActivityList'])\n",
    "psy_usb_Median_df = pd.DataFrame(psyUsbMedianActivityList, columns=['date', 'cluster', 'psyUsbMedianActivityList'])\n",
    "\n",
    "psy_activity_mean_df = pd.DataFrame(psyActivityMeanActivityList, columns=['date', 'cluster', 'psyActivityMeanActivityList'])\n",
    "psy_activity_Median_df = pd.DataFrame(psyActivityMedianActivityList, columns=['date', 'cluster', 'psyActivityMedianActivityList'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, psy_mean_df, on=['date', 'cluster'], how='left')\n",
    "df = pd.merge(df, psy_login_mean_df, on=['date', 'cluster'], how='left')\n",
    "df = pd.merge(df, psy_http_mean_df, on=['date', 'cluster'], how='left')\n",
    "df = pd.merge(df, psy_usb_mean_df, on=['date', 'cluster'], how='left')\n",
    "df = pd.merge(df, psy_activity_mean_df, on=['date', 'cluster'], how='left')\n",
    "df = pd.merge(df, psy_file_mean_df, on=['date', 'cluster'], how='left')\n",
    "df = pd.merge(df, psy_email_mean_df, on=['date', 'cluster'], how='left')\n",
    "\n",
    "df = pd.merge(df, psy_Median_df, on=['date', 'cluster'], how='left')\n",
    "df = pd.merge(df, psy_login_Median_df, on=['date', 'cluster'], how='left')\n",
    "df = pd.merge(df, psy_http_Median_df, on=['date', 'cluster'], how='left')\n",
    "df = pd.merge(df, psy_usb_Median_df, on=['date', 'cluster'], how='left')\n",
    "df = pd.merge(df, psy_activity_Median_df, on=['date', 'cluster'], how='left')\n",
    "df = pd.merge(df, psy_file_Median_df, on=['date', 'cluster'], how='left')\n",
    "df = pd.merge(df, psy_email_Median_df, on=['date', 'cluster'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    roleDist = (np.subtract(np.array(row['activityList']), np.array(row['psyMeanActivityList'])))\n",
    "    A = np.array(row['activityList'])\n",
    "    B = np.array(row['psyMeanActivityList'])\n",
    "    normalized_A = normalize_vector(A)\n",
    "    normalized_B = normalize_vector(B)\n",
    "    cos_sim = cosine_similarity([normalized_A], [normalized_B])\n",
    "    cos_sim = cos_sim[0,0]\n",
    "    euc_dist = euclidean_distances([A], [B])\n",
    "    euc_dist = euc_dist[0,0]\n",
    "    che_dist = chebyshev(A, B)\n",
    "    dot_prod = np.dot(A, B)\n",
    "    user = row['user']\n",
    "    date = row['date']\n",
    "    list_row = {'psy_dist':roleDist, 'psy_cos_sim':cos_sim, 'psy_che_dist':che_dist, 'psy_euc_dist':euc_dist, 'psy_dot_prod':dot_prod, 'user':user, 'date':date}\n",
    "    temp.append(list_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame(temp, columns=['psy_dist', 'psy_cos_sim', 'psy_che_dist', 'psy_euc_dist', 'psy_dot_prod', 'user', 'date'])\n",
    "df = pd.merge(df, df_temp, on=['date', 'user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    roleDist = (np.subtract(np.array(row['loginActivityList']), np.array(row['psyLoginMeanActivityList'])))\n",
    "    A = np.array(row['loginActivityList'])\n",
    "    B = np.array(row['psyLoginMeanActivityList'])\n",
    "    normalized_A = normalize_vector(A)\n",
    "    normalized_B = normalize_vector(B)\n",
    "    cos_sim = cosine_similarity([normalized_A], [normalized_B])\n",
    "    cos_sim = cos_sim[0,0]\n",
    "    euc_dist = euclidean_distances([A], [B])\n",
    "    euc_dist = euc_dist[0,0]\n",
    "    che_dist = chebyshev(A, B)\n",
    "    dot_prod = np.dot(A, B)\n",
    "    user = row['user']\n",
    "    date = row['date']\n",
    "    list_row = {'psy_dist_login':roleDist, 'psy_cos_sim_login':cos_sim, 'psy_che_dist_login':che_dist, 'psy_euc_dist_login':euc_dist, 'psy_dot_prod_login':dot_prod, 'user':user, 'date':date}\n",
    "    temp.append(list_row)\n",
    "df_temp = pd.DataFrame(temp, columns=['psy_dist_login', 'psy_cos_sim_login', 'psy_che_dist_login', 'psy_euc_dist_login', 'psy_dot_prod_login', 'user', 'date'])\n",
    "df = pd.merge(df, df_temp, on=['date', 'user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    roleDist = (np.subtract(np.array(row['httpActivityList']), np.array(row['psyHttpMeanActivityList'])))\n",
    "    A = np.array(row['httpActivityList'])\n",
    "    B = np.array(row['psyHttpMeanActivityList'])\n",
    "    normalized_A = normalize_vector(A)\n",
    "    normalized_B = normalize_vector(B)\n",
    "    cos_sim = cosine_similarity([normalized_A], [normalized_B])\n",
    "    cos_sim = cos_sim[0,0]\n",
    "    euc_dist = euclidean_distances([A], [B])\n",
    "    euc_dist = euc_dist[0,0]\n",
    "    che_dist = chebyshev(A, B)\n",
    "    dot_prod = np.dot(A, B)\n",
    "    user = row['user']\n",
    "    date = row['date']\n",
    "    list_row = {'psy_dist_http':roleDist, 'psy_cos_sim_http':cos_sim, 'psy_che_dist_http':che_dist, 'psy_euc_dist_http':euc_dist, 'psy_dot_prod_http':dot_prod, 'user':user, 'date':date}\n",
    "    temp.append(list_row)\n",
    "df_temp = pd.DataFrame(temp, columns=['psy_dist_http', 'psy_cos_sim_http', 'psy_che_dist_http', 'psy_euc_dist_http', 'psy_dot_prod_http', 'user', 'date'])\n",
    "df = pd.merge(df, df_temp, on=['date', 'user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    roleDist = (np.subtract(np.array(row['emailActivityList']), np.array(row['psyEmailMeanActivityList'])))\n",
    "    A = np.array(row['emailActivityList'])\n",
    "    B = np.array(row['psyEmailMeanActivityList'])\n",
    "    normalized_A = normalize_vector(A)\n",
    "    normalized_B = normalize_vector(B)\n",
    "    cos_sim = cosine_similarity([normalized_A], [normalized_B])\n",
    "    cos_sim = cos_sim[0,0]\n",
    "    euc_dist = euclidean_distances([A], [B])\n",
    "    euc_dist = euc_dist[0,0]\n",
    "    che_dist = chebyshev(A, B)\n",
    "    dot_prod = np.dot(A, B)\n",
    "    user = row['user']\n",
    "    date = row['date']\n",
    "    list_row = {'psy_dist_email':roleDist, 'psy_cos_sim_email':cos_sim, 'psy_che_dist_email':che_dist, 'psy_euc_dist_email':euc_dist, 'psy_dot_prod_email':dot_prod, 'user':user, 'date':date}\n",
    "    temp.append(list_row)\n",
    "df_temp = pd.DataFrame(temp, columns=['psy_dist_email', 'psy_cos_sim_email', 'psy_che_dist_email', 'psy_euc_dist_email', 'psy_dot_prod_email', 'user', 'date'])\n",
    "df = pd.merge(df, df_temp, on=['date', 'user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    roleDist = (np.subtract(np.array(row['fileActivityList']), np.array(row['psyFileMeanActivityList'])))\n",
    "    A = np.array(row['fileActivityList'])\n",
    "    B = np.array(row['psyFileMeanActivityList'])\n",
    "    normalized_A = normalize_vector(A)\n",
    "    normalized_B = normalize_vector(B)\n",
    "    cos_sim = cosine_similarity([normalized_A], [normalized_B])\n",
    "    cos_sim = cos_sim[0,0]\n",
    "    euc_dist = euclidean_distances([A], [B])\n",
    "    euc_dist = euc_dist[0,0]\n",
    "    che_dist = chebyshev(A, B)\n",
    "    dot_prod = np.dot(A, B)\n",
    "    user = row['user']\n",
    "    date = row['date']\n",
    "    list_row = {'psy_dist_file':roleDist, 'psy_cos_sim_file':cos_sim, 'psy_che_dist_file':che_dist, 'psy_euc_dist_file':euc_dist, 'psy_dot_prod_file':dot_prod, 'user':user, 'date':date}\n",
    "    temp.append(list_row)\n",
    "df_temp = pd.DataFrame(temp, columns=['psy_dist_file', 'psy_cos_sim_file', 'psy_che_dist_file', 'psy_euc_dist_file', 'psy_dot_prod_file', 'user', 'date'])\n",
    "\n",
    "df = pd.merge(df, df_temp, on=['date', 'user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    roleDist = (np.subtract(np.array(row['usbActivityList']), np.array(row['psyUsbMeanActivityList'])))\n",
    "    A = np.array(row['usbActivityList'])\n",
    "    B = np.array(row['psyUsbMeanActivityList'])\n",
    "    normalized_A = normalize_vector(A)\n",
    "    normalized_B = normalize_vector(B)\n",
    "    cos_sim = cosine_similarity([normalized_A], [normalized_B])\n",
    "    cos_sim = cos_sim[0,0]\n",
    "    euc_dist = euclidean_distances([A], [B])\n",
    "    euc_dist = euc_dist[0,0]\n",
    "    che_dist = chebyshev(A, B)\n",
    "    dot_prod = np.dot(A, B)\n",
    "    user = row['user']\n",
    "    date = row['date']\n",
    "    list_row = {'psy_dist_usb':roleDist, 'psy_cos_sim_usb':cos_sim, 'psy_che_dist_usb':che_dist, 'psy_euc_dist_usb':euc_dist, 'psy_dot_prod_usb':dot_prod, 'user':user, 'date':date}\n",
    "    temp.append(list_row)\n",
    "df_temp = pd.DataFrame(temp, columns=['psy_dist_usb', 'psy_cos_sim_usb', 'psy_che_dist_usb', 'psy_euc_dist_usb', 'psy_dot_prod_usb', 'user', 'date'])\n",
    "\n",
    "df = pd.merge(df, df_temp, on=['date', 'user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    roleDist = (np.subtract(np.array(row['activityActivityList']), np.array(row['psyActivityMeanActivityList'])))\n",
    "    A = np.array(row['activityActivityList'])\n",
    "    B = np.array(row['psyActivityMeanActivityList'])\n",
    "    normalized_A = normalize_vector(A)\n",
    "    normalized_B = normalize_vector(B)\n",
    "    cos_sim = cosine_similarity([normalized_A], [normalized_B])\n",
    "    cos_sim = cos_sim[0,0]\n",
    "    euc_dist = euclidean_distances([A], [B])\n",
    "    euc_dist = euc_dist[0,0]\n",
    "    che_dist = chebyshev(A, B)\n",
    "    dot_prod = np.dot(A, B)\n",
    "    user = row['user']\n",
    "    date = row['date']\n",
    "    list_row = {'psy_dist_act':roleDist, 'psy_cos_sim_act':cos_sim, 'psy_che_dist_act':che_dist, 'psy_euc_dist_act':euc_dist, 'psy_dot_prod_act':dot_prod, 'user':user, 'date':date}\n",
    "    temp.append(list_row)\n",
    "df_temp = pd.DataFrame(temp, columns=['psy_dist_act', 'psy_cos_sim_act', 'psy_che_dist_act', 'psy_euc_dist_act', 'psy_dot_prod_act', 'user', 'date'])\n",
    "df = pd.merge(df, df_temp, on=['date', 'user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['date', 'user'])\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('tomodels_psy.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import datetime\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score as shs\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold, train_test_split, TimeSeriesSplit\n",
    "import xgboost\n",
    "import shap\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, Dense, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('tomodels_psy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_detected = pd.read_csv('/Users/ken.clements/code/praxis/CERT42/earliest.csv')\n",
    "date_detected['datetime'] = pd.to_datetime(date_detected['date'], infer_datetime_format=True)\n",
    "date_detected['day_date'] = pd.to_datetime(date_detected['datetime']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features =['l1',\n",
    "'l2',\n",
    "'l8',\n",
    "'n_allact',\n",
    "'allact_n-pc0',\n",
    "'allact_n-pc1',\n",
    "'workhourallact_n-pc0',\n",
    "'n_afterhourallact',\n",
    "'n_logon',\n",
    "'n_workhourlogon',\n",
    "'workhourlogon_n-pc0',\n",
    "'n_usb',\n",
    "'usb_n-pc0',\n",
    "'usb_n-pc3',\n",
    "'n_workhourusb',\n",
    "'workhourusb_n-pc0',\n",
    "'afterhourusb_n-pc0',\n",
    "'n_file',\n",
    "'file_n-disk0',\n",
    "'file_n-pc0',\n",
    "'file_n_compf',\n",
    "'file_compf_mean_file_nwords',\n",
    "'file_phof_n-disk0',\n",
    "'file_txtf_mean_file_nwords',\n",
    "'workhourfile_n-disk1',\n",
    "'workhourfile_n_docf',\n",
    "'workhourfile_docf_n-disk0',\n",
    "'afterhourfile_phof_mean_file_nwords',\n",
    "'afterhourfile_phof_n-pc2',\n",
    "'email_mean_n_atts',\n",
    "'email_n-Xemail1',\n",
    "'email_n-exbccmail1',\n",
    "'workhouremail_mean_n_atts',\n",
    "'afterhouremail_mean_n_atts',\n",
    "'afterhouremail_mean_email_size',\n",
    "'afterhouremail_n-exbccmail1',\n",
    "'n_http',\n",
    "'http_mean_url_depth',\n",
    "'http_n_otherf',\n",
    "'http_otherf_mean_url_len',\n",
    "'http_otherf_mean_url_depth',\n",
    "'http_jobf_mean_http_c_len',\n",
    "'http_leakf_n-pc0',\n",
    "'http_n_hackf',\n",
    "'n_workhourhttp',\n",
    "'workhourhttp_mean_url_depth',\n",
    "'workhourhttp_n_otherf',\n",
    "'afterhourhttp_cloudf_mean_http_c_len',\n",
    "'afterhourhttp_jobf_mean_http_c_len',\n",
    "'afterhourhttp_leakf_n-pc0',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_features = ['l1',\n",
    "'l2',\n",
    "'l8',\n",
    "'n_logon',\n",
    "'n_workhourlogon',\n",
    "'workhourlogon_n-pc0']\n",
    "\n",
    "activity_features = ['n_allact',\n",
    "'allact_n-pc0',\n",
    "'allact_n-pc1',\n",
    "'workhourallact_n-pc0',\n",
    "'n_afterhourallact']\n",
    "\n",
    "usb_features = ['n_usb',\n",
    "'usb_n-pc0',\n",
    "'usb_n-pc3',\n",
    "'n_workhourusb',\n",
    "'workhourusb_n-pc0',\n",
    "'afterhourusb_n-pc0']\n",
    "\n",
    "file_features = ['n_file',\n",
    "'file_n-disk0',\n",
    "'file_n-pc0',\n",
    "'file_n_compf',\n",
    "'file_compf_mean_file_nwords',\n",
    "'file_phof_n-disk0',\n",
    "'file_txtf_mean_file_nwords',\n",
    "'workhourfile_n-disk1',\n",
    "'workhourfile_n_docf',\n",
    "'workhourfile_docf_n-disk0',\n",
    "'afterhourfile_phof_mean_file_nwords',\n",
    "'afterhourfile_phof_n-pc2',\n",
    "]\n",
    "\n",
    "email_features = ['email_n-Xemail1',\n",
    "'email_n-exbccmail1',\n",
    "'workhouremail_mean_n_atts',\n",
    "'afterhouremail_mean_n_atts',\n",
    "'afterhouremail_mean_email_size',\n",
    "'afterhouremail_n-exbccmail1',\n",
    "'email_mean_n_atts']\n",
    "\n",
    "http_features = ['n_http',\n",
    "'http_mean_url_depth',\n",
    "'http_n_otherf',\n",
    "'http_otherf_mean_url_len',\n",
    "'http_otherf_mean_url_depth',\n",
    "'http_jobf_mean_http_c_len',\n",
    "'http_leakf_n-pc0',\n",
    "'http_n_hackf',\n",
    "'n_workhourhttp',\n",
    "'workhourhttp_mean_url_depth',\n",
    "'workhourhttp_n_otherf',\n",
    "'afterhourhttp_cloudf_mean_http_c_len',\n",
    "'afterhourhttp_jobf_mean_http_c_len',\n",
    "'afterhourhttp_leakf_n-pc0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_features = []\n",
    "median_features = []\n",
    "iqr_features = []\n",
    "std_features = []\n",
    "\n",
    "for j in best_features:\n",
    "    mean_features.append('rolling_mean_' + j)\n",
    "    median_features.append('rolling_median_' + j)\n",
    "    iqr_features.append('rolling_iqr_' + j)\n",
    "    std_features.append('rolling_std_' + j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_features = mean_features + median_features + std_features + iqr_features\n",
    "\n",
    "# #Standardize\n",
    "# for f in combined_features:\n",
    "#     scaler = StandardScaler()\n",
    "#     train[f] = scaler.fit_transform(np.array(train[f]).reshape(-1,1))\n",
    "#     test[f] = scaler.transform(np.array(test[f]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression()\n",
    "\n",
    "# # Define hyperparameter grid\n",
    "# param_grid = {\n",
    "#     'penalty': ['l2', 'none'],\n",
    "#     'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "# }\n",
    "\n",
    "# # Create GridSearchCV object\n",
    "# lr_best = GridSearchCV(lr, param_grid, cv=10, scoring='recall')\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# lr_best.fit(train[combined_features], train['label'])\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(\"Best hyperparameters:\", lr_best.best_params_)\n",
    "\n",
    "# lr_best_params = lr_best.best_params_\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# y_pred = lr_best.predict(test[combined_features])\n",
    "# recall = recall_score(test['label'], y_pred)\n",
    "# print(\"Best Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best = LogisticRegression(C= 0.001, penalty= 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparameters: {'C': 0.001, 'penalty': 'none'}\n",
    "Best Recall: 0.6533333333333333\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb = xgboost.XGBClassifier()\n",
    "\n",
    "# # Define hyperparameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'learning_rate': [.2, .3, .4],\n",
    "#     'max_depth': [2,4,6,8]\n",
    "# }\n",
    "\n",
    "# # Create GridSearchCV object\n",
    "# xgb_best = GridSearchCV(xgb, param_grid, cv=10, scoring='recall')\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# xgb_best.fit(train[combined_features], train['label'])\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(\"Best hyperparameters:\", xgb_best.best_params_)\n",
    "\n",
    "# xgb_best_params = xgb_best.best_params_\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# y_pred = xgb_best.predict(test[combined_features])\n",
    "# recall = recall_score(test['label'], y_pred)\n",
    "# print(\"Best Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best = xgboost.XGBClassifier(learning_rate= 0.3, max_depth= 6, n_estimators= 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparameters: {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 200}\n",
    "Best Recall: 0.8266666666666667\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = SVC()\n",
    "\n",
    "# # Define hyperparameter grid\n",
    "\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1, 10],\n",
    "#     'gamma': ['auto', 0.1, 1, 10]\n",
    "# }\n",
    "\n",
    "# # Create GridSearchCV object\n",
    "# svm_best = GridSearchCV(svm, param_grid, cv=10, scoring='recall', verbose=3)\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# svm_best.fit(train[combined_features], train['label'])\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(\"Best hyperparameters:\", svm_best.best_params_)\n",
    "\n",
    "# svm_best_params = svm_best.best_params_\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# y_pred = svm_best.predict(test[combined_features])\n",
    "# recall = recall_score(test['label'], y_pred)\n",
    "# print(\"Best Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best = SVC(C = 100, gamma='auto', probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier()\n",
    "\n",
    "# # Define hyperparameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# # Create GridSearchCV object\n",
    "# rf_best = GridSearchCV(rf, param_grid, cv=10, scoring='recall', verbose=3)\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# rf_best.fit(train[combined_features], train['label'])\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(\"Best hyperparameters:\", rf_best.best_params_)\n",
    "\n",
    "# rf_best_params = rf_best.best_params_\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# y_pred = rf_best.predict(test[combined_features])\n",
    "# recall = recall_score(test['label'], y_pred)\n",
    "# print(\"Best Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = RandomForestClassifier(min_samples_leaf=2, min_samples_split=5, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [svm_best, lr_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_features_cos_role = ['cos_sim_login', 'cos_sim_http', 'cos_sim_file', 'cos_sim_usb', 'cos_sim_email']\n",
    "distance_features_euc_role = ['euc_dist_login', 'euc_dist_http', 'euc_dist_file', 'euc_dist_usb', 'euc_dist_email']\n",
    "distance_features_cos_psy = ['psy_cos_sim_login', 'psy_cos_sim_http', 'psy_cos_sim_file', 'psy_cos_sim_usb', 'psy_cos_sim_email']\n",
    "distance_features_euc_psy = ['psy_euc_dist_login', 'psy_euc_dist_http', 'psy_euc_dist_file', 'psy_euc_dist_usb', 'psy_euc_dist_email']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = mean_features + median_features + std_features + iqr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in models:\n",
    "    model_name = n.__class__.__name__\n",
    "    listname = model_name + \"_acc_evaluation_results\"\n",
    "    print(listname)\n",
    "    locals()[listname] = []\n",
    "    listname = model_name + \"_rec_evaluation_results\"\n",
    "    print(listname)\n",
    "    locals()[listname] = []\n",
    "    listname = model_name + \"_prec_evaluation_results\"\n",
    "    print(listname)\n",
    "    locals()[listname] = []\n",
    "    listname = model_name + \"_f1_evaluation_results\"\n",
    "    print(listname)\n",
    "    locals()[listname] = []\n",
    "    listname = model_name + \"_days_till_detected\"\n",
    "    print(listname)\n",
    "    locals()[listname] = []\n",
    "    listname = model_name + \"_cm\"\n",
    "    print(listname)\n",
    "    locals()[listname] = []\n",
    "    listname = model_name + \"_roc_auc_evaluation_results\"\n",
    "    print(listname)\n",
    "    locals()[listname] = []\n",
    "    listname = model_name + \"_auc_pr_evaluation_results\"\n",
    "    print(listname)\n",
    "    locals()[listname] = []\n",
    "    listname = model_name + \"_fpr_evaluation_results\"\n",
    "    print(listname)\n",
    "    locals()[listname] = []\n",
    "    listname = model_name + \"_tpr_evaluation_results\"\n",
    "    print(listname)\n",
    "    locals()[listname] = []\n",
    "\n",
    "train_start = datetime.date(2010,6,14)\n",
    "train_end = datetime.date(2011,4,4)\n",
    "\n",
    "\n",
    "cluster_period = df.copy()\n",
    "cluster_period['date'] = pd.to_datetime(cluster_period['date']).dt.date\n",
    "cluster_period = cluster_period.loc[(cluster_period['date'] >= train_start) & (cluster_period['date'] <= train_end)]\n",
    "\n",
    "for m in models:\n",
    "\n",
    "    model_name = m.__class__.__name__\n",
    "\n",
    "    caught = []\n",
    "\n",
    "    n_splits = 21\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    for train_index, val_index in tscv.split(cluster_period):\n",
    "        # try:\n",
    "        # Split the data into training and validation sets\n",
    "        X_train, X_val = cluster_period.iloc[train_index], cluster_period.iloc[val_index]\n",
    "        y_train, y_val = cluster_period['label'].iloc[train_index], cluster_period['label'].iloc[val_index]\n",
    "\n",
    "        #Standardize\n",
    "        for f in combined_features:\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train[f] = scaler.fit_transform(np.array(X_train[f]).reshape(-1,1))\n",
    "            X_val[f] = scaler.transform(np.array(X_val[f]).reshape(-1,1))\n",
    "\n",
    "        m.fit(X_train[combined_features], y_train)\n",
    "        y_pred = m.predict(X_val[combined_features])\n",
    "\n",
    "        y_prob = m.predict_proba(X_val[combined_features])[:, 1]\n",
    "        fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
    "        roc_auc = roc_auc_score(y_val, y_prob)\n",
    "        precision, recall, thresholds = precision_recall_curve(y_val, y_prob)\n",
    "        auc_pr = auc(recall, precision)\n",
    "\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # plt.plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "        # plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "        # plt.xlabel('False Positive Rate (FPR)')\n",
    "        # plt.ylabel('True Positive Rate (TPR)')\n",
    "        # plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        # plt.legend(loc='lower right')\n",
    "        # plt.show()\n",
    "\n",
    "        X_val['pred'] = y_pred\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "        listname = model_name + \"_acc_evaluation_results\"\n",
    "        locals()[listname].append(accuracy)\n",
    "        listname = model_name + \"_rec_evaluation_results\"\n",
    "        locals()[listname].append(recall)\n",
    "        listname = model_name + \"_prec_evaluation_results\"\n",
    "        locals()[listname].append(precision)\n",
    "        listname = model_name + \"_f1_evaluation_results\"\n",
    "        locals()[listname].append(f1)\n",
    "\n",
    "        cf_matrix = confusion_matrix(y_val, y_pred)\n",
    "        print(cf_matrix)\n",
    "\n",
    "        listname = model_name + \"_cm\"\n",
    "        locals()[listname].append(cf_matrix)\n",
    "\n",
    "        listname = model_name + \"_roc_auc_evaluation_results\"\n",
    "        locals()[listname].append(roc_auc)\n",
    "\n",
    "        listname = model_name + \"_auc_pr_evaluation_results\"\n",
    "        locals()[listname].append(auc_pr)\n",
    "\n",
    "        listname = model_name + \"_fpr_evaluation_results\"\n",
    "        locals()[listname].append(fpr)\n",
    "\n",
    "        listname = model_name + \"_tpr_evaluation_results\"\n",
    "        locals()[listname].append(tpr)\n",
    "\n",
    "        listname = model_name + \"_days_till_detected\"\n",
    "\n",
    "        for index, row in X_val.iterrows():\n",
    "            if ((row['label'] == 1) & ((row['pred'] == 1))):\n",
    "                if row['user'] in caught:\n",
    "                    continue\n",
    "                print(\"Caught Bad Actor: \" + row['user'])\n",
    "                date = date_detected['day_date'].loc[date_detected['user'] == row['user']]\n",
    "                caught.append(row['user'])\n",
    "                locals()[listname].append(row['user'] + \",\" + str(row['date'] - date))\n",
    "        # except:\n",
    "        #     print(\"Problem with \" + model_name)\n",
    "        #     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = []\n",
    "for m in models:\n",
    "    model_names.append(m.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model_names:\n",
    "\n",
    "        listname_df = m + \"_df\"\n",
    "        listname_destected_df = m + \"_days_till_detected_df\"\n",
    "\n",
    "        listname_df_csv = m + \"_df.csv\"\n",
    "        listname_destected_df_csv = m + \"_days_till_detected_df.csv\"\n",
    "\n",
    "        listname_detected = m + \"_days_till_detected\"\n",
    "        listname_acc = m + \"_acc_evaluation_results\"\n",
    "        listname_rec = m + \"_rec_evaluation_results\"\n",
    "        listname_prec = m + \"_prec_evaluation_results\"\n",
    "        listname_f1 = m + \"_f1_evaluation_results\"\n",
    "        listname_roc = m + \"_roc_auc_evaluation_results\"\n",
    "        listname_pr = m + \"_auc_pr_evaluation_results\"\n",
    "        listname_tpr = m + \"_tpr_evaluation_results\"\n",
    "        listname_fpr = m + \"_fpr_evaluation_results\"\n",
    "        listname_cm = m + \"_cm\"\n",
    "\n",
    "        listname_detected_csv = m + \"_days_till_detected.csv\"\n",
    "        listname_acc_csv = m + \"_acc_evaluation_results.csv\"\n",
    "        listname_rec_csv = m + \"_rec_evaluation_results.csv\"\n",
    "        listname_prec_csv = m + \"_prec_evaluation_results.csv\"\n",
    "        listname_f1_csv = m + \"_f1_evaluation_results.csv\"\n",
    "        listname_roc_csv = m + \"_roc_auc_evaluation_results.csv\"\n",
    "        listname_pr_csv = m + \"_auc_pr_evaluation_results.csv\"\n",
    "        listname_tpr_csv = m + \"_tpr_evaluation_results.csv\"\n",
    "        listname_fpr_csv = m + \"_fpr_evaluation_results.csv\"\n",
    "        listname_cm_csv = m + \"_cm\"\n",
    "\n",
    "        data = {\n",
    "            'Acc': locals()[listname_acc],\n",
    "            'Rec': locals()[listname_rec],\n",
    "            'Prec': locals()[listname_prec],\n",
    "            'F1': locals()[listname_f1],\n",
    "            'ROC': locals()[listname_roc],\n",
    "            'PR': locals()[listname_pr],\n",
    "            'Acc': locals()[listname_acc],\n",
    "            'CM': locals()[listname_cm],\n",
    "            'tpr': locals()[listname_tpr],\n",
    "            'fpr': locals()[listname_fpr],\n",
    "        }\n",
    "\n",
    "        locals()[listname_df] = pd.DataFrame(data)\n",
    "        locals()[listname_df].to_csv(listname_df_csv, index=False)\n",
    "\n",
    "        locals()[listname_destected_df] = pd.DataFrame(locals()[listname_detected])\n",
    "        locals()[listname_destected_df].to_csv(listname_destected_df_csv, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
